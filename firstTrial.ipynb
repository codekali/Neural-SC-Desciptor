{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/codekali/Neural-SC-Desciptor/blob/master/firstTrial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "Gcmj8Cb9a4aN",
    "outputId": "9272ad71-ea8f-4eb1-c918-f9b3e4f9c5bb"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/content/drive/My Drive/Transformer/train/train.token.sbt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-db5d9889d29a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mcount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mfile\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/drive/My Drive/Transformer/train/train.token.sbt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m   \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/My Drive/Transformer/train/train.token.sbt'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import json\n",
    "from torch.utils.data import Dataset\n",
    "dictionary={}\n",
    "X,Y = [],[]\n",
    "count=-1   \n",
    "file=open(\"/content/drive/My Drive/Transformer/train/train.token.sbt\")\n",
    "for line in file:\n",
    "  X.append(line)\n",
    "  count+=1\n",
    "count=0\n",
    "file=open(\"/content/drive/My Drive/Transformer/train/train.token.nl\")\n",
    "for line in file:\n",
    "  Y.append(line)\n",
    "  count+=1\n",
    "print(len(Y))\n",
    "print(len(X))\n",
    "X=X[0:1000]\n",
    "Y=Y[0:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "LSCj9GE0EDLy",
    "outputId": "e69044c3-8299-4297-fb55-b974de41feb8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000\n",
      "20000\n"
     ]
    }
   ],
   "source": [
    "# You can even put this data load operation in your Dataset class\n",
    "import torch\n",
    "import json\n",
    "from torch.utils.data import Dataset\n",
    "dictionary={}\n",
    "X_valid,Y_valid = [],[]\n",
    "count=-1   \n",
    "file=open(\"/content/drive/My Drive/Transformer/val/valid.token.sbt\")\n",
    "for line in file:\n",
    "  X_valid.append(line)\n",
    "  count+=1\n",
    "count=0\n",
    "file=open(\"/content/drive/My Drive/Transformer/val/valid.token.nl\")\n",
    "for line in file:\n",
    "  Y_valid.append(line)\n",
    "  count+=1\n",
    "print(len(Y_valid))\n",
    "print(len(X_valid))\n",
    "X_valid=X_valid[0:100]\n",
    "Y_valid=Y_valid[0:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "E65v5JOxdj99",
    "outputId": "8575abdb-3ca0-46e7-98f8-c1b31f700ed8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "print(len(Y))\n",
    "print(len(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2swvO7qqbAb6"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils import data\n",
    "\n",
    "class Dataset(data.Dataset):\n",
    "  def __init__(self, X_item, Y_item):\n",
    "    self.X_item=X_item\n",
    "    self.Y_item=Y_item\n",
    "    # No need for this\n",
    "    self.device=device\n",
    "\n",
    "  def __len__(self):\n",
    "        'Denotes the total number of samples'\n",
    "        return len(self.X_item)\n",
    "\n",
    "  def __getitem__(self, index):\n",
    "    X = self.X_item[index]\n",
    "    Y = self.Y_item[index]\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "4jFm-_ywbY7z",
    "outputId": "1d47afbc-f1e8-4180-a133-17a77bdc689a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "#import mydata\n",
    "import torch\n",
    "from torch.utils import data\n",
    "# CUDA for PyTorch\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "print(device)\n",
    "#cudann.benchmark = True\n",
    "\n",
    "# Parameters\n",
    "params = {'batch_size': 5,\n",
    "          'shuffle': True,\n",
    "          'num_workers': 6}\n",
    "max_epochs = 10\n",
    "\n",
    "training_set = Dataset(X,Y)\n",
    "training_generator = data.DataLoader(training_set, **params)\n",
    "\n",
    "validation_set = Dataset(X_valid,Y_valid)\n",
    "validation_generator = data.DataLoader(validation_set, **params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "7LgliBxibZKp",
    "outputId": "75d1862e-354a-4670-ebed-d7355a13381c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installing collected packages: tokenizers, sentencepiece, sacremoses, transformers\n",
      "Successfully installed sacremoses-0.0.41 sentencepiece-0.1.86 tokenizers-0.5.2 transformers-2.8.0\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 527
    },
    "colab_type": "code",
    "id": "6zQaAWNlcBK-",
    "outputId": "9a7bf882-c56b-4f25-8881-80037f0bb6da"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch  0  completed!, Training LOSS is:  tensor(20.8415, device='cuda:0', grad_fn=<NllLossBackward>)  Validation loss is:  tensor(10.0547, device='cuda:0')\n",
      "\n",
      "Epoch  1  completed!, Training LOSS is:  tensor(11.9161, device='cuda:0', grad_fn=<NllLossBackward>)  Validation loss is:  tensor(16.9352, device='cuda:0')\n",
      "\n",
      "Epoch  2  completed!, Training LOSS is:  tensor(18.0669, device='cuda:0', grad_fn=<NllLossBackward>)  Validation loss is:  tensor(19.1240, device='cuda:0')\n",
      "\n",
      "Epoch  3  completed!, Training LOSS is:  tensor(20.4952, device='cuda:0', grad_fn=<NllLossBackward>)  Validation loss is:  tensor(19.1648, device='cuda:0')\n",
      "\n",
      "Epoch  4  completed!, Training LOSS is:  tensor(16.0663, device='cuda:0', grad_fn=<NllLossBackward>)  Validation loss is:  tensor(16.6882, device='cuda:0')\n",
      "\n",
      "Epoch  5  completed!, Training LOSS is:  tensor(12.5693, device='cuda:0', grad_fn=<NllLossBackward>)  Validation loss is:  tensor(17.0689, device='cuda:0')\n",
      "\n",
      "Epoch  6  completed!, Training LOSS is:  tensor(22.4999, device='cuda:0', grad_fn=<NllLossBackward>)  Validation loss is:  tensor(15.9427, device='cuda:0')\n",
      "\n",
      "Epoch  7  completed!, Training LOSS is:  tensor(10.9391, device='cuda:0', grad_fn=<NllLossBackward>)  Validation loss is:  tensor(7.6967, device='cuda:0')\n",
      "\n",
      "Epoch  8  completed!, Training LOSS is:  tensor(17.2467, device='cuda:0', grad_fn=<NllLossBackward>)  Validation loss is:  tensor(13.2819, device='cuda:0')\n",
      "\n",
      "Epoch  9  completed!, Training LOSS is:  tensor(21.0187, device='cuda:0', grad_fn=<NllLossBackward>)  Validation loss is:  tensor(12.9119, device='cuda:0')\n",
      "\n",
      "Epoch  10  completed!, Training LOSS is:  tensor(9.2573, device='cuda:0', grad_fn=<NllLossBackward>)  Validation loss is:  tensor(19.0192, device='cuda:0')\n",
      "\n",
      "Epoch  11  completed!, Training LOSS is:  tensor(13.6382, device='cuda:0', grad_fn=<NllLossBackward>)  Validation loss is:  tensor(12.0085, device='cuda:0')\n",
      "\n",
      "Epoch  12  completed!, Training LOSS is:  tensor(20.1329, device='cuda:0', grad_fn=<NllLossBackward>)  Validation loss is:  tensor(19.7693, device='cuda:0')\n",
      "\n",
      "Epoch  13  completed!, Training LOSS is:  tensor(26.6254, device='cuda:0', grad_fn=<NllLossBackward>)  Validation loss is:  tensor(22.6960, device='cuda:0')\n",
      "\n",
      "Epoch  14  completed!, Training LOSS is:  tensor(10.9508, device='cuda:0', grad_fn=<NllLossBackward>)  Validation loss is:  tensor(7.8972, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "import torch.optim as optim\n",
    "\n",
    "tokenizer = T5Tokenizer.from_pretrained('t5-small')\n",
    "model = T5ForConditionalGeneration.from_pretrained('t5-small')\n",
    "model = model.to(device)\n",
    "# optimizer should be defined outside training loop\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "for epoch in range(15):\n",
    "    trainloss=0\n",
    "    validloss=0\n",
    "    # Training\n",
    "    for local_batch, local_labels in training_generator: \n",
    "        \"\"\"Transfer local_batch and labels to GPU\"\"\"\n",
    "        #local_batch, local_labels = local_batch.to(device), local_labels.to(device)\n",
    "        \"\"\" You can write the above statement as:\n",
    "        local_batch=(x.to(device) for x in local_batch)\n",
    "        local_labels=(x.to(device) for x in local_labels)\n",
    "        But I have transfered them to the device after encoding them\n",
    "        \"\"\"\n",
    "        optimizer.zero_grad()\n",
    "        \"\"\"Forward Function Implementation\"\"\"\n",
    "        # batch_encode_plus also returns a mask, try passing that mask as well in the model.\n",
    "        # Refer - https://github.com/huggingface/transformers/blob/a79a9e1241b9257bbce1d933fee47f3a8ff90797/examples/translation/t5/evaluate_wmt.py#L17\n",
    "        # Also, put the encode function in dataset. Return encoded matrix values from dataset. Here, only take it to gpu.\n",
    "        input_ids = tokenizer.batch_encode_plus(local_batch, return_tensors=\"pt\",pad_to_max_length=True)\n",
    "        label = tokenizer.batch_encode_plus(local_labels, return_tensors=\"pt\",pad_to_max_length=True)\n",
    "        outputs = model(input_ids=(input_ids['input_ids']).to(device), lm_labels=(label['input_ids']).to(device))\n",
    "        loss, prediction_scores = outputs[:2]\n",
    "        trainloss=loss\n",
    "        #print(loss)\n",
    "        \"\"\"Forward Function Ends here\"\"\"\n",
    "        \"\"\"Loss and optimizer\"\"\"\n",
    "        # No you shouldn't do model zero grad. This will reset model.\n",
    "        # model.zero_grad()\n",
    "        loss.backward()\n",
    "        # If you define optimizer here, new optimizer will be declared every step - no training will happen.\n",
    "        # optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "        # optimizer.zero_grad() should be before loss.backward. Ideally at the top of training loop. Before model.\n",
    "        # optimizer.zero_grad()\n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "    \"\"\"Validation\"\"\"\n",
    "    with torch.set_grad_enabled(False):\n",
    "        for local_batch, local_labels in validation_generator:\n",
    "          input_ids = tokenizer.batch_encode_plus(local_batch, return_tensors=\"pt\",pad_to_max_length=True)\n",
    "          label = tokenizer.batch_encode_plus(local_labels, return_tensors=\"pt\",pad_to_max_length=True)\n",
    "          model.eval()\n",
    "          outputs = model(input_ids=(input_ids['input_ids']).to(device), lm_labels=(label['input_ids']).to(device))\n",
    "          loss, prediction_scores = outputs[:2]\n",
    "          validloss=loss\n",
    "    print(\"\\nEpoch \", epoch, \" completed!, Training LOSS is: \", trainloss, \" Validation loss is: \", validloss)\n",
    "    #print(loss, \"\\n\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VMdN-HAVDuGm"
   },
   "outputs": [],
   "source": [
    " model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oXLsw4qsNmPq"
   },
   "outputs": [],
   "source": [
    "from datetime import datetime as dt\n",
    "runtime =  dt.now()\n",
    "model_name = \"/content/drive/My Drive/Transformer/models/\" + now.strftime(\"%d/%m/%Y %H:%M\") + \".pth\"\n",
    "\n",
    "torch.save(model.state_dict(), model_name)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyNajIPpYSDzocxkVmIaXB+p",
   "collapsed_sections": [],
   "include_colab_link": true,
   "mount_file_id": "1nhPCWeEdpyWpxOTSVCFr6j7UW9sdNjeO",
   "name": "firstTrial.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
