{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "firstTrial.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.2"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/codekali/Neural-SC-Desciptor/blob/master/firstTrial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "1Q5Y0eA2_bnH",
        "outputId": "b41dda54-a855-493b-8bd1-51857e1f9768",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 126
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Gcmj8Cb9a4aN",
        "outputId": "30bfaef3-d63c-4719-b2e3-e01c11ef8386",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "import torch\n",
        "import json\n",
        "from torch.utils.data import Dataset\n",
        "dictionary={}\n",
        "X,Y = [],[]\n",
        "count=-1   \n",
        "file=open(\"/content/drive/My Drive/Transformer/train/train.token.sbt\")\n",
        "for line in file:\n",
        "  X.append(line)\n",
        "  count+=1\n",
        "count=0\n",
        "file=open(\"/content/drive/My Drive/Transformer/train/train.token.nl\")\n",
        "for line in file:\n",
        "  Y.append(line)\n",
        "  count+=1\n",
        "print(len(Y))\n",
        "print(len(X))\n",
        "X=X[0:100]\n",
        "Y=Y[0:100]"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "445812\n",
            "445812\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "LSCj9GE0EDLy",
        "outputId": "f547d952-0c0e-4aa5-fc90-6fc7670e75c5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "# You can even put this data load operation in your Dataset class\n",
        "import torch\n",
        "import json\n",
        "from torch.utils.data import Dataset\n",
        "dictionary={}\n",
        "X_valid,Y_valid = [],[]\n",
        "count=-1   \n",
        "file=open(\"/content/drive/My Drive/Transformer/val/valid.token.sbt\")\n",
        "for line in file:\n",
        "  X_valid.append(line)\n",
        "  count+=1\n",
        "count=0\n",
        "file=open(\"/content/drive/My Drive/Transformer/val/valid.token.nl\")\n",
        "for line in file:\n",
        "  Y_valid.append(line)\n",
        "  count+=1\n",
        "print(len(Y_valid))\n",
        "print(len(X_valid))\n",
        "X_valid=X_valid[0:20]\n",
        "Y_valid=Y_valid[0:20]"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "20000\n",
            "20000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "E65v5JOxdj99",
        "outputId": "6202b2d2-ff23-4ada-b840-d173de2c66b6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "print(len(Y))\n",
        "print(len(X))"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100\n",
            "100\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "2swvO7qqbAb6",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "from torch.utils import data\n",
        "\n",
        "class Dataset(data.Dataset):\n",
        "  def __init__(self, X_item, Y_item):\n",
        "    self.X_item=X_item\n",
        "    self.Y_item=Y_item\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.X_item)\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    X = self.X_item[index]\n",
        "    \n",
        "    Y = self.Y_item[index]\n",
        "    return X, Y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "4jFm-_ywbY7z",
        "outputId": "1d7c5abd-eb8d-4ad2-9751-7686231662b6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "#import mydata\n",
        "import torch\n",
        "from torch.utils import data\n",
        "use_cuda = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "print(device)\n",
        "params = {'batch_size': 5,\n",
        "          'shuffle': True,\n",
        "          'num_workers': 6}\n",
        "max_epochs = 10\n",
        "\n",
        "training_set = Dataset(X,Y)\n",
        "training_generator = data.DataLoader(training_set, **params)\n",
        "\n",
        "validation_set = Dataset(X_valid,Y_valid)\n",
        "validation_generator = data.DataLoader(validation_set, **params)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "7LgliBxibZKp",
        "scrolled": true,
        "outputId": "08f5a24b-8cfa-4de2-8a98-41445a2cb349",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 448
        }
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (2.8.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.38.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.21.0)\n",
            "Requirement already satisfied: tokenizers==0.5.2 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.5.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (from transformers) (0.1.86)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.3)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from transformers) (1.12.43)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.41)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.4.5.1)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.8)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.9.5)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.3.3)\n",
            "Requirement already satisfied: botocore<1.16.0,>=1.15.43 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (1.15.43)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.12.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.14.1)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.16.0,>=1.15.43->boto3->transformers) (2.8.1)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.16.0,>=1.15.43->boto3->transformers) (0.15.2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5B1p1kvrJGM3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# coding=utf-8\n",
        "# Copyright 2018 The Google AI Language Team Authors and The HuggingFace Inc. team.\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "\"\"\"PyTorch optimization for BERT model.\"\"\"\n",
        "\n",
        "import math\n",
        "import torch\n",
        "from torch.optim import Optimizer\n",
        "from torch.optim.optimizer import required\n",
        "from torch.nn.utils import clip_grad_norm_\n",
        "import logging\n",
        "import abc\n",
        "import sys\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "\n",
        "if sys.version_info >= (3, 4):\n",
        "    ABC = abc.ABC\n",
        "else:\n",
        "    ABC = abc.ABCMeta('ABC', (), {})\n",
        "\n",
        "\n",
        "class _LRSchedule(ABC):\n",
        "    \"\"\" Parent of all LRSchedules here. \"\"\"\n",
        "    warn_t_total = False        # is set to True for schedules where progressing beyond t_total steps doesn't make sense\n",
        "    def __init__(self, warmup=0.002, t_total=-1, **kw):\n",
        "        \"\"\"\n",
        "        :param warmup:  what fraction of t_total steps will be used for linear warmup\n",
        "        :param t_total: how many training steps (updates) are planned\n",
        "        :param kw:\n",
        "        \"\"\"\n",
        "        super(_LRSchedule, self).__init__(**kw)\n",
        "        if t_total < 0:\n",
        "            logger.warning(\"t_total value of {} results in schedule not being applied\".format(t_total))\n",
        "        if not 0.0 <= warmup < 1.0 and not warmup == -1:\n",
        "            raise ValueError(\"Invalid warmup: {} - should be in [0.0, 1.0[ or -1\".format(warmup))\n",
        "        warmup = max(warmup, 0.)\n",
        "        self.warmup, self.t_total = float(warmup), float(t_total)\n",
        "        self.warned_for_t_total_at_progress = -1\n",
        "\n",
        "    def get_lr(self, step, nowarn=False):\n",
        "        \"\"\"\n",
        "        :param step:    which of t_total steps we're on\n",
        "        :param nowarn:  set to True to suppress warning regarding training beyond specified 't_total' steps\n",
        "        :return:        learning rate multiplier for current update\n",
        "        \"\"\"\n",
        "        if self.t_total < 0:\n",
        "            return 1.\n",
        "        progress = float(step) / self.t_total\n",
        "        ret = self.get_lr_(progress)\n",
        "        # warning for exceeding t_total (only active with warmup_linear\n",
        "        if not nowarn and self.warn_t_total and progress > 1. and progress > self.warned_for_t_total_at_progress:\n",
        "            logger.warning(\n",
        "                \"Training beyond specified 't_total'. Learning rate multiplier set to {}. Please set 't_total' of {} correctly.\"\n",
        "                    .format(ret, self.__class__.__name__))\n",
        "            self.warned_for_t_total_at_progress = progress\n",
        "        # end warning\n",
        "        return ret\n",
        "\n",
        "    @abc.abstractmethod\n",
        "    def get_lr_(self, progress):\n",
        "        \"\"\"\n",
        "        :param progress:    value between 0 and 1 (unless going beyond t_total steps) specifying training progress\n",
        "        :return:            learning rate multiplier for current update\n",
        "        \"\"\"\n",
        "        return 1.\n",
        "\n",
        "\n",
        "class ConstantLR(_LRSchedule):\n",
        "    def get_lr_(self, progress):\n",
        "        return 1.\n",
        "\n",
        "\n",
        "class WarmupCosineSchedule(_LRSchedule):\n",
        "    \"\"\"\n",
        "    Linearly increases learning rate from 0 to 1 over `warmup` fraction of training steps.\n",
        "    Decreases learning rate from 1. to 0. over remaining `1 - warmup` steps following a cosine curve.\n",
        "    If `cycles` (default=0.5) is different from default, learning rate follows cosine function after warmup.\n",
        "    \"\"\"\n",
        "    warn_t_total = True\n",
        "    def __init__(self, warmup=0.002, t_total=-1, cycles=.5, **kw):\n",
        "        \"\"\"\n",
        "        :param warmup:      see LRSchedule\n",
        "        :param t_total:     see LRSchedule\n",
        "        :param cycles:      number of cycles. Default: 0.5, corresponding to cosine decay from 1. at progress==warmup and 0 at progress==1.\n",
        "        :param kw:\n",
        "        \"\"\"\n",
        "        super(WarmupCosineSchedule, self).__init__(warmup=warmup, t_total=t_total, **kw)\n",
        "        self.cycles = cycles\n",
        "\n",
        "    def get_lr_(self, progress):\n",
        "        if progress < self.warmup:\n",
        "            return progress / self.warmup\n",
        "        else:\n",
        "            progress = (progress - self.warmup) / (1 - self.warmup)   # progress after warmup\n",
        "            return 0.5 * (1. + math.cos(math.pi * self.cycles * 2 * progress))\n",
        "\n",
        "\n",
        "class WarmupCosineWithHardRestartsSchedule(WarmupCosineSchedule):\n",
        "    \"\"\"\n",
        "    Linearly increases learning rate from 0 to 1 over `warmup` fraction of training steps.\n",
        "    If `cycles` (default=1.) is different from default, learning rate follows `cycles` times a cosine decaying\n",
        "    learning rate (with hard restarts).\n",
        "    \"\"\"\n",
        "    def __init__(self, warmup=0.002, t_total=-1, cycles=1., **kw):\n",
        "        super(WarmupCosineWithHardRestartsSchedule, self).__init__(warmup=warmup, t_total=t_total, cycles=cycles, **kw)\n",
        "        assert(cycles >= 1.)\n",
        "\n",
        "    def get_lr_(self, progress):\n",
        "        if progress < self.warmup:\n",
        "            return progress / self.warmup\n",
        "        else:\n",
        "            progress = (progress - self.warmup) / (1 - self.warmup)     # progress after warmup\n",
        "            ret = 0.5 * (1. + math.cos(math.pi * ((self.cycles * progress) % 1)))\n",
        "            return ret\n",
        "\n",
        "\n",
        "class WarmupCosineWithWarmupRestartsSchedule(WarmupCosineWithHardRestartsSchedule):\n",
        "    \"\"\"\n",
        "    All training progress is divided in `cycles` (default=1.) parts of equal length.\n",
        "    Every part follows a schedule with the first `warmup` fraction of the training steps linearly increasing from 0. to 1.,\n",
        "    followed by a learning rate decreasing from 1. to 0. following a cosine curve.\n",
        "    \"\"\"\n",
        "    def __init__(self, warmup=0.002, t_total=-1, cycles=1., **kw):\n",
        "        assert(warmup * cycles < 1.)\n",
        "        warmup = warmup * cycles if warmup >= 0 else warmup\n",
        "        super(WarmupCosineWithWarmupRestartsSchedule, self).__init__(warmup=warmup, t_total=t_total, cycles=cycles, **kw)\n",
        "\n",
        "    def get_lr_(self, progress):\n",
        "        progress = progress * self.cycles % 1.\n",
        "        if progress < self.warmup:\n",
        "            return progress / self.warmup\n",
        "        else:\n",
        "            progress = (progress - self.warmup) / (1 - self.warmup)     # progress after warmup\n",
        "            ret = 0.5 * (1. + math.cos(math.pi * progress))\n",
        "            return ret\n",
        "\n",
        "\n",
        "class WarmupConstantSchedule(_LRSchedule):\n",
        "    \"\"\"\n",
        "    Linearly increases learning rate from 0 to 1 over `warmup` fraction of training steps.\n",
        "    Keeps learning rate equal to 1. after warmup.\n",
        "    \"\"\"\n",
        "    def get_lr_(self, progress):\n",
        "        if progress < self.warmup:\n",
        "            return progress / self.warmup\n",
        "        return 1.\n",
        "\n",
        "\n",
        "class WarmupLinearSchedule(_LRSchedule):\n",
        "    \"\"\"\n",
        "    Linearly increases learning rate from 0 to 1 over `warmup` fraction of training steps.\n",
        "    Linearly decreases learning rate from 1. to 0. over remaining `1 - warmup` steps.\n",
        "    \"\"\"\n",
        "    warn_t_total = True\n",
        "    def get_lr_(self, progress):\n",
        "        if progress < self.warmup:\n",
        "            return progress / self.warmup\n",
        "        return max((progress - 1.) / (self.warmup - 1.), 0.)\n",
        "\n",
        "\n",
        "SCHEDULES = {\n",
        "    None:       ConstantLR,\n",
        "    \"none\":     ConstantLR,\n",
        "    \"warmup_cosine\": WarmupCosineSchedule,\n",
        "    \"warmup_constant\": WarmupConstantSchedule,\n",
        "    \"warmup_linear\": WarmupLinearSchedule\n",
        "}\n",
        "\n",
        "\n",
        "class BertAdam(Optimizer):\n",
        "    \"\"\"Implements BERT version of Adam algorithm with weight decay fix.\n",
        "    Params:\n",
        "        lr: learning rate\n",
        "        warmup: portion of t_total for the warmup, -1  means no warmup. Default: -1\n",
        "        t_total: total number of training steps for the learning\n",
        "            rate schedule, -1  means constant learning rate of 1. (no warmup regardless of warmup setting). Default: -1\n",
        "        schedule: schedule to use for the warmup (see above).\n",
        "            Can be `'warmup_linear'`, `'warmup_constant'`, `'warmup_cosine'`, `'none'`, `None` or a `_LRSchedule` object (see below).\n",
        "            If `None` or `'none'`, learning rate is always kept constant.\n",
        "            Default : `'warmup_linear'`\n",
        "        b1: Adams b1. Default: 0.9\n",
        "        b2: Adams b2. Default: 0.999\n",
        "        e: Adams epsilon. Default: 1e-6\n",
        "        weight_decay: Weight decay. Default: 0.01\n",
        "        max_grad_norm: Maximum norm for the gradients (-1 means no clipping). Default: 1.0\n",
        "    \"\"\"\n",
        "    def __init__(self, params, lr=required, warmup=-1, t_total=-1, schedule='warmup_linear',\n",
        "                 b1=0.9, b2=0.999, e=1e-6, weight_decay=0.01, max_grad_norm=1.0, **kwargs):\n",
        "        if lr is not required and lr < 0.0:\n",
        "            raise ValueError(\"Invalid learning rate: {} - should be >= 0.0\".format(lr))\n",
        "        if not isinstance(schedule, _LRSchedule) and schedule not in SCHEDULES:\n",
        "            raise ValueError(\"Invalid schedule parameter: {}\".format(schedule))\n",
        "        if not 0.0 <= b1 < 1.0:\n",
        "            raise ValueError(\"Invalid b1 parameter: {} - should be in [0.0, 1.0[\".format(b1))\n",
        "        if not 0.0 <= b2 < 1.0:\n",
        "            raise ValueError(\"Invalid b2 parameter: {} - should be in [0.0, 1.0[\".format(b2))\n",
        "        if not e >= 0.0:\n",
        "            raise ValueError(\"Invalid epsilon value: {} - should be >= 0.0\".format(e))\n",
        "        # initialize schedule object\n",
        "        if not isinstance(schedule, _LRSchedule):\n",
        "            schedule_type = SCHEDULES[schedule]\n",
        "            schedule = schedule_type(warmup=warmup, t_total=t_total)\n",
        "        else:\n",
        "            if warmup != -1 or t_total != -1:\n",
        "                logger.warning(\"warmup and t_total on the optimizer are ineffective when _LRSchedule object is provided as schedule. \"\n",
        "                               \"Please specify custom warmup and t_total in _LRSchedule object.\")\n",
        "        defaults = dict(lr=lr, schedule=schedule,\n",
        "                        b1=b1, b2=b2, e=e, weight_decay=weight_decay,\n",
        "                        max_grad_norm=max_grad_norm)\n",
        "        self.rate = None\n",
        "        super(BertAdam, self).__init__(params, defaults)\n",
        "\n",
        "    def show_lr(self):\n",
        "        return self.rate\n",
        "\n",
        "    def get_lr(self):\n",
        "        lr = []\n",
        "        for group in self.param_groups:\n",
        "            for p in group['params']:\n",
        "                state = self.state[p]\n",
        "                if len(state) == 0:\n",
        "                    return [0]\n",
        "                lr_scheduled = group['lr']\n",
        "                lr_scheduled *= group['schedule'].get_lr(state['step'])\n",
        "                lr.append(lr_scheduled)\n",
        "        return lr\n",
        "\n",
        "    def step(self, closure=None):\n",
        "        \"\"\"Performs a single optimization step.\n",
        "        Arguments:\n",
        "            closure (callable, optional): A closure that reevaluates the model\n",
        "                and returns the loss.\n",
        "        \"\"\"\n",
        "        loss = None\n",
        "        if closure is not None:\n",
        "            loss = closure()\n",
        "\n",
        "        for group in self.param_groups:\n",
        "            for p in group['params']:\n",
        "                if p.grad is None:\n",
        "                    continue\n",
        "                grad = p.grad.data\n",
        "                if grad.is_sparse:\n",
        "                    raise RuntimeError('Adam does not support sparse gradients, please consider SparseAdam instead')\n",
        "\n",
        "                state = self.state[p]\n",
        "\n",
        "                # State initialization\n",
        "                if len(state) == 0:\n",
        "                    state['step'] = 0\n",
        "                    # Exponential moving average of gradient values\n",
        "                    state['next_m'] = torch.zeros_like(p.data)\n",
        "                    # Exponential moving average of squared gradient values\n",
        "                    state['next_v'] = torch.zeros_like(p.data)\n",
        "\n",
        "                next_m, next_v = state['next_m'], state['next_v']\n",
        "                beta1, beta2 = group['b1'], group['b2']\n",
        "\n",
        "                # Add grad clipping\n",
        "                if group['max_grad_norm'] > 0:\n",
        "                    clip_grad_norm_(p, group['max_grad_norm'])\n",
        "\n",
        "                # Decay the first and second moment running average coefficient\n",
        "                # In-place operations to update the averages at the same time\n",
        "                next_m.mul_(beta1).add_(1 - beta1, grad)\n",
        "                next_v.mul_(beta2).addcmul_(1 - beta2, grad, grad)\n",
        "                update = next_m / (next_v.sqrt() + group['e'])\n",
        "\n",
        "                # Just adding the square of the weights to the loss function is *not*\n",
        "                # the correct way of using L2 regularization/weight decay with Adam,\n",
        "                # since that will interact with the m and v parameters in strange ways.\n",
        "                #\n",
        "                # Instead we want to decay the weights in a manner that doesn't interact\n",
        "                # with the m/v parameters. This is equivalent to adding the square\n",
        "                # of the weights to the loss with plain (non-momentum) SGD.\n",
        "                if group['weight_decay'] > 0.0:\n",
        "                    update += group['weight_decay'] * p.data\n",
        "\n",
        "                lr_scheduled = group['lr']\n",
        "                lr_scheduled *= group['schedule'].get_lr(state['step'])\n",
        "\n",
        "                self.rate = lr_scheduled\n",
        "\n",
        "                update_with_lr = lr_scheduled * update\n",
        "                p.data.add_(-update_with_lr)\n",
        "\n",
        "                state['step'] += 1\n",
        "\n",
        "                # step_size = lr_scheduled * math.sqrt(bias_correction2) / bias_correction1\n",
        "                # No bias correction\n",
        "                # bias_correction1 = 1 - beta1 ** state['step']\n",
        "                # bias_correction2 = 1 - beta2 ** state['step']\n",
        "\n",
        "        return loss\n",
        "\n",
        "\n",
        "class Adam(Optimizer):\n",
        "    \"\"\"Implements pytorch version of Adam algorithm with weight decay fix.\n",
        "    Params:\n",
        "        lr: learning rate\n",
        "        warmup: portion of t_total for the warmup, -1  means no warmup. Default: -1\n",
        "        t_total: total number of training steps for the learning\n",
        "            rate schedule, -1  means constant learning rate of 1. (no warmup regardless of warmup setting). Default: -1\n",
        "        schedule: schedule to use for the warmup (see above).\n",
        "            Can be `'warmup_linear'`, `'warmup_constant'`, `'warmup_cosine'`, `'none'`, `None` or a `_LRSchedule` object (see below).\n",
        "            If `None` or `'none'`, learning rate is always kept constant.\n",
        "            Default : `'warmup_linear'`\n",
        "        b1: Adams b1. Default: 0.9\n",
        "        b2: Adams b2. Default: 0.999\n",
        "        e: Adams epsilon. Default: 1e-6\n",
        "        weight_decay: Weight decay. Default: 0.01\n",
        "        max_grad_norm: Maximum norm for the gradients (-1 means no clipping). Default: 1.0\n",
        "    \"\"\"\n",
        "    def __init__(self, params, lr=required, warmup=-1, t_total=-1, schedule='warmup_linear',\n",
        "                 b1=0.9, b2=0.999, e=1e-8, weight_decay=0,  amsgrad=False, max_grad_norm=1.0, **kwargs):\n",
        "        if lr is not required and lr < 0.0:\n",
        "            raise ValueError(\"Invalid learning rate: {} - should be >= 0.0\".format(lr))\n",
        "        if not isinstance(schedule, _LRSchedule) and schedule not in SCHEDULES:\n",
        "            raise ValueError(\"Invalid schedule parameter: {}\".format(schedule))\n",
        "        if not 0.0 <= b1 < 1.0:\n",
        "            raise ValueError(\"Invalid b1 parameter: {} - should be in [0.0, 1.0[\".format(b1))\n",
        "        if not 0.0 <= b2 < 1.0:\n",
        "            raise ValueError(\"Invalid b2 parameter: {} - should be in [0.0, 1.0[\".format(b2))\n",
        "        if not e >= 0.0:\n",
        "            raise ValueError(\"Invalid epsilon value: {} - should be >= 0.0\".format(e))\n",
        "        # initialize schedule object\n",
        "        if not isinstance(schedule, _LRSchedule):\n",
        "            schedule_type = SCHEDULES[schedule]\n",
        "            schedule = schedule_type(warmup=warmup, t_total=t_total)\n",
        "        else:\n",
        "            if warmup != -1 or t_total != -1:\n",
        "                logger.warning(\"warmup and t_total on the optimizer are ineffective when _LRSchedule object is provided as schedule. \"\n",
        "                               \"Please specify custom warmup and t_total in _LRSchedule object.\")\n",
        "        defaults = dict(lr=lr, schedule=schedule,\n",
        "                        b1=b1, b2=b2, e=e, weight_decay=weight_decay,\n",
        "                        amsgrad=amsgrad, max_grad_norm=max_grad_norm)\n",
        "        self.rate = None\n",
        "        super(Adam, self).__init__(params, defaults)\n",
        "\n",
        "    def __setstate__(self, state):\n",
        "        super(Adam, self).__setstate__(state)\n",
        "        for group in self.param_groups:\n",
        "            group.setdefault('amsgrad', False)\n",
        "\n",
        "    def show_lr(self):\n",
        "        return self.rate\n",
        "\n",
        "    def get_lr(self):\n",
        "        lr = []\n",
        "        for group in self.param_groups:\n",
        "            for p in group['params']:\n",
        "                state = self.state[p]\n",
        "                if len(state) == 0:\n",
        "                    return [0]\n",
        "                lr_scheduled = group['lr']\n",
        "                lr_scheduled *= group['schedule'].get_lr(state['step'])\n",
        "                lr.append(lr_scheduled)\n",
        "        return lr\n",
        "\n",
        "    def step(self, closure=None):\n",
        "        \"\"\"Performs a single optimization step.\n",
        "        Arguments:\n",
        "            closure (callable, optional): A closure that reevaluates the model\n",
        "                and returns the loss.\n",
        "        \"\"\"\n",
        "        loss = None\n",
        "        if closure is not None:\n",
        "            loss = closure()\n",
        "\n",
        "        for group in self.param_groups:\n",
        "            for p in group['params']:\n",
        "                if p.grad is None:\n",
        "                    continue\n",
        "                grad = p.grad.data\n",
        "                if grad.is_sparse:\n",
        "                    raise RuntimeError('Adam does not support sparse gradients, please consider SparseAdam instead')\n",
        "                amsgrad = group['amsgrad']\n",
        "\n",
        "                state = self.state[p]\n",
        "\n",
        "                # State initialization\n",
        "                if len(state) == 0:\n",
        "                    state['step'] = 0\n",
        "                    # Exponential moving average of gradient values\n",
        "                    state['exp_avg'] = torch.zeros_like(p.data)\n",
        "                    # Exponential moving average of squared gradient values\n",
        "                    state['exp_avg_sq'] = torch.zeros_like(p.data)\n",
        "                    if amsgrad:\n",
        "                        # Maintains max of all exp. moving avg. of sq. grad. values\n",
        "                        state['max_exp_avg_sq'] = torch.zeros_like(p.data)\n",
        "\n",
        "                exp_avg, exp_avg_sq = state['exp_avg'], state['exp_avg_sq']\n",
        "                if amsgrad:\n",
        "                    max_exp_avg_sq = state['max_exp_avg_sq']\n",
        "                beta1, beta2 = group['b1'], group['b2']\n",
        "\n",
        "                # Add grad clipping\n",
        "                if group['max_grad_norm'] > 0:\n",
        "                    clip_grad_norm_(p, group['max_grad_norm'])\n",
        "\n",
        "                lr_scheduled = group['lr']\n",
        "                lr_scheduled *= group['schedule'].get_lr(state['step'])\n",
        "                self.rate = lr_scheduled\n",
        "\n",
        "                state['step'] += 1\n",
        "\n",
        "                if group['weight_decay'] != 0:\n",
        "                    grad.add_(group['weight_decay'], p.data)\n",
        "\n",
        "                # Decay the first and second moment running average coefficient\n",
        "                exp_avg.mul_(beta1).add_(1 - beta1, grad)\n",
        "                exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\n",
        "                if amsgrad:\n",
        "                    # Maintains the maximum of all 2nd moment running avg. till now\n",
        "                    torch.max(max_exp_avg_sq, exp_avg_sq, out=max_exp_avg_sq)\n",
        "                    # Use the max. for normalizing running avg. of gradient\n",
        "                    denom = max_exp_avg_sq.sqrt().add_(group['e'])\n",
        "                else:\n",
        "                    denom = exp_avg_sq.sqrt().add_(group['e'])\n",
        "\n",
        "                bias_correction1 = 1 - beta1 ** state['step']\n",
        "                bias_correction2 = 1 - beta2 ** state['step']\n",
        "                step_size = lr_scheduled * math.sqrt(bias_correction2) / bias_correction1\n",
        "\n",
        "                p.data.addcdiv_(-step_size, exp_avg, denom)\n",
        "\n",
        "        return loss\n",
        "\n",
        "class Adamax(Optimizer):\n",
        "    \"\"\"Implements Adamax algorithm (a variant of Adam based on infinity norm).\n",
        "    It has been proposed in `Adam: A Method for Stochastic Optimization`__.\n",
        "    Arguments:\n",
        "        params (iterable): iterable of parameters to optimize or dicts defining\n",
        "            parameter groups\n",
        "        lr (float, optional): learning rate (default: 2e-3)\n",
        "        betas (Tuple[float, float], optional): coefficients used for computing\n",
        "            running averages of gradient and its square\n",
        "        eps (float, optional): term added to the denominator to improve\n",
        "            numerical stability (default: 1e-8)\n",
        "        weight_decay (float, optional): weight decay (L2 penalty) (default: 0)\n",
        "    __ https://arxiv.org/abs/1412.6980\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, params, lr=required, warmup=-1, t_total=-1, schedule='warmup_linear',\n",
        "                 b1=0.9, b2=0.999, e=1e-8, weight_decay=0, max_grad_norm=1.0, **kwargs):\n",
        "        if not 0.0 <= lr:\n",
        "            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n",
        "        if not 0.0 <= e:\n",
        "            raise ValueError(\"Invalid epsilon value: {}\".format(e))\n",
        "        if not 0.0 <= b1 < 1.0:\n",
        "            raise ValueError(\"Invalid b1 parameter: {} - should be in [0.0, 1.0[\".format(b1))\n",
        "        if not 0.0 <= b2 < 1.0:\n",
        "            raise ValueError(\"Invalid b2 parameter: {} - should be in [0.0, 1.0[\".format(b2))\n",
        "        if not 0.0 <= weight_decay:\n",
        "            raise ValueError(\"Invalid weight_decay value: {}\".format(weight_decay))\n",
        "\n",
        "        if not isinstance(schedule, _LRSchedule):\n",
        "            schedule_type = SCHEDULES[schedule]            \n",
        "            schedule = schedule_type(warmup=warmup, t_total=t_total)\n",
        "        else:\n",
        "            if warmup != -1 or t_total != -1:\n",
        "                logger.warning(\"warmup and t_total on the optimizer are ineffective when _LRSchedule object is provided as schedule. \"\n",
        "                               \"Please specify custom warmup and t_total in _LRSchedule object.\")\n",
        "\n",
        "        defaults = dict(lr=lr, schedule=schedule,\n",
        "                        b1=b1, b2=b2, e=e, weight_decay=weight_decay, max_grad_norm=max_grad_norm)\n",
        "        self.rate = None\n",
        "        super(Adamax, self).__init__(params, defaults)\n",
        "\n",
        "    def show_lr(self):\n",
        "        return self.rate\n",
        "\n",
        "    def get_lr(self):\n",
        "        lr = []\n",
        "        for group in self.param_groups:\n",
        "            for p in group['params']:\n",
        "                state = self.state[p]\n",
        "                if len(state) == 0:\n",
        "                    return [0]\n",
        "                lr_scheduled = group['lr']\n",
        "                lr_scheduled *= group['schedule'].get_lr(state['step'])\n",
        "                lr.append(lr_scheduled)\n",
        "        return lr\n",
        "\n",
        "    def step(self, closure=None):\n",
        "        \"\"\"Performs a single optimization step.\n",
        "        Arguments:\n",
        "            closure (callable, optional): A closure that reevaluates the model\n",
        "                and returns the loss.\n",
        "        \"\"\"\n",
        "        loss = None\n",
        "        if closure is not None:\n",
        "            loss = closure()\n",
        "\n",
        "        for group in self.param_groups:\n",
        "            for p in group['params']:\n",
        "                if p.grad is None:\n",
        "                    continue\n",
        "                grad = p.grad.data\n",
        "                if grad.is_sparse:\n",
        "                    raise RuntimeError('Adamax does not support sparse gradients')\n",
        "                state = self.state[p]\n",
        "\n",
        "                # State initialization\n",
        "                if len(state) == 0:\n",
        "                    state['step'] = 0\n",
        "                    state['exp_avg'] = torch.zeros_like(p.data)\n",
        "                    state['exp_inf'] = torch.zeros_like(p.data)\n",
        "\n",
        "                exp_avg, exp_inf = state['exp_avg'], state['exp_inf']\n",
        "                beta1, beta2 = group['b1'], group['b2']\n",
        "                eps = group['e']\n",
        "\n",
        "                # Add grad clipping\n",
        "                if group['max_grad_norm'] > 0:\n",
        "                    clip_grad_norm_(p, group['max_grad_norm'])\n",
        "\n",
        "                lr_scheduled = group['lr']\n",
        "                lr_scheduled *= group['schedule'].get_lr(state['step'])\n",
        "\n",
        "                self.rate = lr_scheduled\n",
        "                state['step'] += 1\n",
        "\n",
        "                if group['weight_decay'] != 0:\n",
        "                    grad = grad.add(group['weight_decay'], p.data)\n",
        "\n",
        "                # Update biased first moment estimate.\n",
        "                exp_avg.mul_(beta1).add_(1 - beta1, grad)\n",
        "                # Update the exponentially weighted infinity norm.\n",
        "                norm_buf = torch.cat([\n",
        "                    exp_inf.mul_(beta2).unsqueeze(0),\n",
        "                    grad.abs().add_(eps).unsqueeze_(0)\n",
        "                ], 0)\n",
        "                torch.max(norm_buf, 0, keepdim=False, out=(exp_inf, exp_inf.new().long()))\n",
        "\n",
        "                bias_correction = 1 - beta1 ** state['step']\n",
        "                clr = lr_scheduled / bias_correction\n",
        "\n",
        "                p.data.addcdiv_(-clr, exp_avg, exp_inf)\n",
        "\n",
        "        return loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "14Qm-iHYJGM-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "tokenizer = T5Tokenizer.from_pretrained('t5-small')\n",
        "model = T5ForConditionalGeneration.from_pretrained('t5-small')\n",
        "model = model.to(device)\n",
        "# optimizer should be defined outside training loop\n",
        "# optimizer = optim.SGD(model.parameters(), lr=0.01)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VScZuyPaJGNF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "num_epoch = 20\n",
        "no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n",
        "optimizer_grouped_parameters = []\n",
        "lr=2e-5\n",
        "for key, value in dict(model.named_parameters()).items():\n",
        "    if value.requires_grad:\n",
        "        if any(nd in key for nd in no_decay):\n",
        "            optimizer_grouped_parameters += [\n",
        "                {\"params\": [value], \"lr\": lr, \"weight_decay\": 0.01}\n",
        "            ]\n",
        "        if not any(nd in key for nd in no_decay):\n",
        "            optimizer_grouped_parameters += [\n",
        "                {\"params\": [value], \"lr\": lr, \"weight_decay\": 0.0}\n",
        "            ]\n",
        "                \n",
        "optimizer = BertAdam(\n",
        "            optimizer_grouped_parameters,\n",
        "            lr=lr,\n",
        "            warmup=0.1,\n",
        "            t_total=100,\n",
        "#             t_total=num_epoch * len(training_generator),\n",
        "            schedule='warmup_constant',\n",
        "        )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "6zQaAWNlcBK-",
        "outputId": "b2995113-c159-4961-c0a5-9f2f50bf735b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 752
        }
      },
      "source": [
        "\n",
        "loss_train = []\n",
        "loss_valid = []\n",
        "validloss = 0\n",
        "for local_batch, local_labels in validation_generator:\n",
        "    input_ids = tokenizer.batch_encode_plus(local_batch, return_tensors=\"pt\",pad_to_max_length=True)\n",
        "    label = tokenizer.batch_encode_plus(local_labels, return_tensors=\"pt\",pad_to_max_length=True)\n",
        "    model.eval()\n",
        "    outputs = model(input_ids=(input_ids['input_ids']).to(device), lm_labels=(label['input_ids']).to(device))\n",
        "    loss, prediction_scores = outputs[:2]\n",
        "    validloss += loss\n",
        "loss_valid.append(validloss / 4)\n",
        "\n",
        "for epoch in range(num_epoch):\n",
        "    trainloss=0\n",
        "    validloss=0\n",
        "    # Training\n",
        "    for local_batch, local_labels in training_generator: \n",
        "        model.zero_grad()\n",
        "        \"\"\"Forward Function Implementation\"\"\"\n",
        "        # Also, put the encode function in dataset. Return encoded matrix values from dataset. Here, only take it to gpu.\n",
        "        input_ids = tokenizer.batch_encode_plus(local_batch, return_tensors=\"pt\",pad_to_max_length=True)\n",
        "        label = tokenizer.batch_encode_plus(local_labels, return_tensors=\"pt\",pad_to_max_length=True)\n",
        "        outputs = model(input_ids=(input_ids['input_ids']).to(device), lm_labels=(label['input_ids']).to(device),attention_mask=(input_ids['attention_mask']).to(device))\n",
        "        loss, prediction_scores = outputs[:2]\n",
        "        trainloss=loss\n",
        "        \"\"\"Forward Function Ends here\"\"\"\n",
        "        loss_train.append(trainloss)\n",
        "        \"\"\"Loss and optimizer\"\"\"\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    \n",
        "\n",
        "\n",
        "    \"\"\"Validation\"\"\"\n",
        "    with torch.set_grad_enabled(False):\n",
        "        for local_batch, local_labels in validation_generator:\n",
        "          input_ids = tokenizer.batch_encode_plus(local_batch, return_tensors=\"pt\",pad_to_max_length=True)\n",
        "          label = tokenizer.batch_encode_plus(local_labels, return_tensors=\"pt\",pad_to_max_length=True)\n",
        "          model.eval()\n",
        "          outputs = model(input_ids=(input_ids['input_ids']).to(device), lm_labels=(label['input_ids']).to(device))\n",
        "          loss, prediction_scores = outputs[:2]\n",
        "          validloss += loss\n",
        "    loss_valid.append(validloss / 4)\n",
        "    print(\"\\nEpoch \", epoch, \" completed!, Training LOSS is: \", trainloss, \" Validation loss is: \", validloss/4)\n",
        "#plt.plot(loss_train)\n",
        "#plt.plot(loss_valid)\n",
        "    #print(loss, \"\\n\\n\")\n"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch  0  completed!, Training LOSS is:  tensor(3.9901, device='cuda:0', grad_fn=<NllLossBackward>)  Validation loss is:  tensor(18.1304, device='cuda:0')\n",
            "\n",
            "Epoch  1  completed!, Training LOSS is:  tensor(2.9545, device='cuda:0', grad_fn=<NllLossBackward>)  Validation loss is:  tensor(14.5982, device='cuda:0')\n",
            "\n",
            "Epoch  2  completed!, Training LOSS is:  tensor(2.2266, device='cuda:0', grad_fn=<NllLossBackward>)  Validation loss is:  tensor(20.4818, device='cuda:0')\n",
            "\n",
            "Epoch  3  completed!, Training LOSS is:  tensor(3.3734, device='cuda:0', grad_fn=<NllLossBackward>)  Validation loss is:  tensor(16.8403, device='cuda:0')\n",
            "\n",
            "Epoch  4  completed!, Training LOSS is:  tensor(3.1351, device='cuda:0', grad_fn=<NllLossBackward>)  Validation loss is:  tensor(19.6312, device='cuda:0')\n",
            "\n",
            "Epoch  5  completed!, Training LOSS is:  tensor(2.7221, device='cuda:0', grad_fn=<NllLossBackward>)  Validation loss is:  tensor(19.8100, device='cuda:0')\n",
            "\n",
            "Epoch  6  completed!, Training LOSS is:  tensor(2.4877, device='cuda:0', grad_fn=<NllLossBackward>)  Validation loss is:  tensor(18.2500, device='cuda:0')\n",
            "\n",
            "Epoch  7  completed!, Training LOSS is:  tensor(3.2456, device='cuda:0', grad_fn=<NllLossBackward>)  Validation loss is:  tensor(21.1510, device='cuda:0')\n",
            "\n",
            "Epoch  8  completed!, Training LOSS is:  tensor(2.9225, device='cuda:0', grad_fn=<NllLossBackward>)  Validation loss is:  tensor(18.7052, device='cuda:0')\n",
            "\n",
            "Epoch  9  completed!, Training LOSS is:  tensor(2.3186, device='cuda:0', grad_fn=<NllLossBackward>)  Validation loss is:  tensor(20.9647, device='cuda:0')\n",
            "\n",
            "Epoch  10  completed!, Training LOSS is:  tensor(2.2556, device='cuda:0', grad_fn=<NllLossBackward>)  Validation loss is:  tensor(18.8625, device='cuda:0')\n",
            "\n",
            "Epoch  11  completed!, Training LOSS is:  tensor(2.5599, device='cuda:0', grad_fn=<NllLossBackward>)  Validation loss is:  tensor(20.3774, device='cuda:0')\n",
            "\n",
            "Epoch  12  completed!, Training LOSS is:  tensor(1.7179, device='cuda:0', grad_fn=<NllLossBackward>)  Validation loss is:  tensor(18.8030, device='cuda:0')\n",
            "\n",
            "Epoch  13  completed!, Training LOSS is:  tensor(1.5068, device='cuda:0', grad_fn=<NllLossBackward>)  Validation loss is:  tensor(20.4469, device='cuda:0')\n",
            "\n",
            "Epoch  14  completed!, Training LOSS is:  tensor(1.2755, device='cuda:0', grad_fn=<NllLossBackward>)  Validation loss is:  tensor(19.1761, device='cuda:0')\n",
            "\n",
            "Epoch  15  completed!, Training LOSS is:  tensor(1.2953, device='cuda:0', grad_fn=<NllLossBackward>)  Validation loss is:  tensor(21.4953, device='cuda:0')\n",
            "\n",
            "Epoch  16  completed!, Training LOSS is:  tensor(1.2598, device='cuda:0', grad_fn=<NllLossBackward>)  Validation loss is:  tensor(18.7546, device='cuda:0')\n",
            "\n",
            "Epoch  17  completed!, Training LOSS is:  tensor(1.4238, device='cuda:0', grad_fn=<NllLossBackward>)  Validation loss is:  tensor(19.3818, device='cuda:0')\n",
            "\n",
            "Epoch  18  completed!, Training LOSS is:  tensor(1.5279, device='cuda:0', grad_fn=<NllLossBackward>)  Validation loss is:  tensor(19.0503, device='cuda:0')\n",
            "\n",
            "Epoch  19  completed!, Training LOSS is:  tensor(0.7523, device='cuda:0', grad_fn=<NllLossBackward>)  Validation loss is:  tensor(19.1857, device='cuda:0')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5pxbcJIEhUcL",
        "colab_type": "code",
        "outputId": "e97a7bc6-cbf4-4604-aebb-35f471c46380",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 531
        }
      },
      "source": [
        "plt.plot(loss_train)\n",
        "plt.show()\n",
        "plt.plot(loss_valid)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7fcdf8a526a0>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "VMdN-HAVDuGm",
        "colab": {}
      },
      "source": [
        " model.eval()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "pVOZGflPBMIk",
        "outputId": "a953e9a2-68e9-48b4-a740-60782d11a4a3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "temp_index=50 \n",
        "ip=tokenizer.encode(X[temp_index], return_tensors=\"pt\")\n",
        "op=model.generate(input_ids=ip.to(device))\n",
        "print(tokenizer.decode(op[0]))\n",
        "print(X[temp_index])\n",
        "print(Y[temp_index])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "creates an instance of the factory.\n",
            "( ConstructorDeclaration ( StatementExpression ( SuperConstructorInvocation ) SuperConstructorInvocation ) StatementExpression ) ConstructorDeclaration\n",
            "\n",
            "creates an instance of the factory .\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "oXLsw4qsNmPq",
        "colab": {}
      },
      "source": [
        "from datetime import datetime as dt\n",
        "runtime =  dt.now()\n",
        "model_name = \"/content/drive/My Drive/Transformer/models/\" + runtime.strftime(\"%d:%m:%Y:%H:%M\") + \".pth\"\n",
        "\n",
        "torch.save(model.state_dict(), model_name)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}