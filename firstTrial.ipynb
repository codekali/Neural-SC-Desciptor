{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/codekali/Neural-SC-Desciptor/blob/master/firstTrial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "Gcmj8Cb9a4aN",
    "outputId": "6114c251-dc93-4862-c8d4-2598c8f7f60a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "445812\n",
      "445812\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import json\n",
    "from torch.utils.data import Dataset\n",
    "dictionary={}\n",
    "X,Y = [],[]\n",
    "count=-1   \n",
    "file=open(\"/content/drive/My Drive/Transformer/train/train.token.sbt\")\n",
    "for line in file:\n",
    "  X.append(line)\n",
    "  count+=1\n",
    "count=0\n",
    "file=open(\"/content/drive/My Drive/Transformer/train/train.token.nl\")\n",
    "for line in file:\n",
    "  Y.append(line)\n",
    "  count+=1\n",
    "print(len(Y))\n",
    "print(len(X))\n",
    "X=X[0:100]\n",
    "Y=Y[0:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "LSCj9GE0EDLy",
    "outputId": "e5634628-50a8-47e8-a530-2f7521bf5ea2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000\n",
      "20000\n"
     ]
    }
   ],
   "source": [
    "# You can even put this data load operation in your Dataset class\n",
    "import torch\n",
    "import json\n",
    "from torch.utils.data import Dataset\n",
    "dictionary={}\n",
    "X_valid,Y_valid = [],[]\n",
    "count=-1   \n",
    "file=open(\"/content/drive/My Drive/Transformer/val/valid.token.sbt\")\n",
    "for line in file:\n",
    "  X_valid.append(line)\n",
    "  count+=1\n",
    "count=0\n",
    "file=open(\"/content/drive/My Drive/Transformer/val/valid.token.nl\")\n",
    "for line in file:\n",
    "  Y_valid.append(line)\n",
    "  count+=1\n",
    "print(len(Y_valid))\n",
    "print(len(X_valid))\n",
    "X_valid=X_valid[0:20]\n",
    "Y_valid=Y_valid[0:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "E65v5JOxdj99"
   },
   "outputs": [],
   "source": [
    "print(len(Y))\n",
    "print(len(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2swvO7qqbAb6"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils import data\n",
    "\n",
    "class Dataset(data.Dataset):\n",
    "  def __init__(self, X_item, Y_item):\n",
    "    self.X_item=X_item\n",
    "    self.Y_item=Y_item\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.X_item)\n",
    "\n",
    "  def __getitem__(self, index):\n",
    "    X = self.X_item[index]\n",
    "    \n",
    "    Y = self.Y_item[index]\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "4jFm-_ywbY7z",
    "outputId": "b8d1e8b3-3703-455a-cd72-21a79712665c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "#import mydata\n",
    "import torch\n",
    "from torch.utils import data\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "print(device)\n",
    "params = {'batch_size': 5,\n",
    "          'shuffle': True,\n",
    "          'num_workers': 6}\n",
    "max_epochs = 10\n",
    "\n",
    "training_set = Dataset(X,Y)\n",
    "training_generator = data.DataLoader(training_set, **params)\n",
    "\n",
    "validation_set = Dataset(X_valid,Y_valid)\n",
    "validation_generator = data.DataLoader(validation_set, **params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7LgliBxibZKp",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Downloading transformers-2.8.0-py3-none-any.whl (563 kB)\n",
      "\u001b[K     |████████████████████████████████| 563 kB 1.3 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting sentencepiece\n",
      "  Downloading sentencepiece-0.1.86-cp38-cp38-manylinux1_x86_64.whl (1.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.0 MB 3.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting requests\n",
      "  Downloading requests-2.23.0-py2.py3-none-any.whl (58 kB)\n",
      "\u001b[K     |████████████████████████████████| 58 kB 2.8 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting regex!=2019.12.17\n",
      "  Downloading regex-2020.4.4-cp38-cp38-manylinux2010_x86_64.whl (691 kB)\n",
      "\u001b[K     |████████████████████████████████| 691 kB 5.4 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tqdm>=4.27\n",
      "  Downloading tqdm-4.45.0-py2.py3-none-any.whl (60 kB)\n",
      "\u001b[K     |████████████████████████████████| 60 kB 3.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting sacremoses\n",
      "  Downloading sacremoses-0.0.41.tar.gz (883 kB)\n",
      "\u001b[K     |████████████████████████████████| 883 kB 18.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting boto3\n",
      "  Downloading boto3-1.12.46-py2.py3-none-any.whl (128 kB)\n",
      "\u001b[K     |████████████████████████████████| 128 kB 3.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting filelock\n",
      "  Downloading filelock-3.0.12-py3-none-any.whl (7.6 kB)\n",
      "Requirement already satisfied: numpy in /home/hammad/anaconda3/envs/torchtest/lib/python3.8/site-packages (from transformers) (1.18.1)\n",
      "Collecting tokenizers==0.5.2\n",
      "  Downloading tokenizers-0.5.2-cp38-cp38-manylinux1_x86_64.whl (7.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 7.4 MB 4.9 MB/s eta 0:00:011\n",
      "\u001b[?25hCollecting chardet<4,>=3.0.2\n",
      "  Downloading chardet-3.0.4-py2.py3-none-any.whl (133 kB)\n",
      "\u001b[K     |████████████████████████████████| 133 kB 5.2 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1\n",
      "  Downloading urllib3-1.25.9-py2.py3-none-any.whl (126 kB)\n",
      "\u001b[K     |████████████████████████████████| 126 kB 3.5 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: certifi>=2017.4.17 in /home/hammad/anaconda3/envs/torchtest/lib/python3.8/site-packages (from requests->transformers) (2020.4.5.1)\n",
      "Collecting idna<3,>=2.5\n",
      "  Downloading idna-2.9-py2.py3-none-any.whl (58 kB)\n",
      "\u001b[K     |████████████████████████████████| 58 kB 4.1 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: six in /home/hammad/anaconda3/envs/torchtest/lib/python3.8/site-packages (from sacremoses->transformers) (1.14.0)\n",
      "Collecting click\n",
      "  Downloading click-7.1.1-py2.py3-none-any.whl (82 kB)\n",
      "\u001b[K     |████████████████████████████████| 82 kB 2.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting joblib\n",
      "  Downloading joblib-0.14.1-py2.py3-none-any.whl (294 kB)\n",
      "\u001b[K     |████████████████████████████████| 294 kB 6.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting jmespath<1.0.0,>=0.7.1\n",
      "  Downloading jmespath-0.9.5-py2.py3-none-any.whl (24 kB)\n",
      "Collecting botocore<1.16.0,>=1.15.46\n",
      "  Downloading botocore-1.15.46-py2.py3-none-any.whl (6.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 6.1 MB 2.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting s3transfer<0.4.0,>=0.3.0\n",
      "  Downloading s3transfer-0.3.3-py2.py3-none-any.whl (69 kB)\n",
      "\u001b[K     |████████████████████████████████| 69 kB 2.3 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting docutils<0.16,>=0.10\n",
      "  Downloading docutils-0.15.2-py3-none-any.whl (547 kB)\n",
      "\u001b[K     |████████████████████████████████| 547 kB 2.3 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: python-dateutil<3.0.0,>=2.1 in /home/hammad/anaconda3/envs/torchtest/lib/python3.8/site-packages (from botocore<1.16.0,>=1.15.46->boto3->transformers) (2.8.1)\n",
      "Building wheels for collected packages: sacremoses\n",
      "  Building wheel for sacremoses (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for sacremoses: filename=sacremoses-0.0.41-py3-none-any.whl size=893334 sha256=b9686e977ffd11e74f072135bf2874cf6196b5644385cbaff6aa8300345db76b\n",
      "  Stored in directory: /home/hammad/.cache/pip/wheels/e8/bf/be/5b1e04a1be41b94063ced461b9b8aadd9ba625d0d60fd7d176\n",
      "Successfully built sacremoses\n",
      "Installing collected packages: sentencepiece, chardet, urllib3, idna, requests, regex, tqdm, click, joblib, sacremoses, jmespath, docutils, botocore, s3transfer, boto3, filelock, tokenizers, transformers\n",
      "Successfully installed boto3-1.12.46 botocore-1.15.46 chardet-3.0.4 click-7.1.1 docutils-0.15.2 filelock-3.0.12 idna-2.9 jmespath-0.9.5 joblib-0.14.1 regex-2020.4.4 requests-2.23.0 s3transfer-0.3.3 sacremoses-0.0.41 sentencepiece-0.1.86 tokenizers-0.5.2 tqdm-4.45.0 transformers-2.8.0 urllib3-1.25.9\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding=utf-8\n",
    "# Copyright 2018 The Google AI Language Team Authors and The HuggingFace Inc. team.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\"\"\"PyTorch optimization for BERT model.\"\"\"\n",
    "\n",
    "import math\n",
    "import torch\n",
    "from torch.optim import Optimizer\n",
    "from torch.optim.optimizer import required\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "import logging\n",
    "import abc\n",
    "import sys\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "if sys.version_info >= (3, 4):\n",
    "    ABC = abc.ABC\n",
    "else:\n",
    "    ABC = abc.ABCMeta('ABC', (), {})\n",
    "\n",
    "\n",
    "class _LRSchedule(ABC):\n",
    "    \"\"\" Parent of all LRSchedules here. \"\"\"\n",
    "    warn_t_total = False        # is set to True for schedules where progressing beyond t_total steps doesn't make sense\n",
    "    def __init__(self, warmup=0.002, t_total=-1, **kw):\n",
    "        \"\"\"\n",
    "        :param warmup:  what fraction of t_total steps will be used for linear warmup\n",
    "        :param t_total: how many training steps (updates) are planned\n",
    "        :param kw:\n",
    "        \"\"\"\n",
    "        super(_LRSchedule, self).__init__(**kw)\n",
    "        if t_total < 0:\n",
    "            logger.warning(\"t_total value of {} results in schedule not being applied\".format(t_total))\n",
    "        if not 0.0 <= warmup < 1.0 and not warmup == -1:\n",
    "            raise ValueError(\"Invalid warmup: {} - should be in [0.0, 1.0[ or -1\".format(warmup))\n",
    "        warmup = max(warmup, 0.)\n",
    "        self.warmup, self.t_total = float(warmup), float(t_total)\n",
    "        self.warned_for_t_total_at_progress = -1\n",
    "\n",
    "    def get_lr(self, step, nowarn=False):\n",
    "        \"\"\"\n",
    "        :param step:    which of t_total steps we're on\n",
    "        :param nowarn:  set to True to suppress warning regarding training beyond specified 't_total' steps\n",
    "        :return:        learning rate multiplier for current update\n",
    "        \"\"\"\n",
    "        if self.t_total < 0:\n",
    "            return 1.\n",
    "        progress = float(step) / self.t_total\n",
    "        ret = self.get_lr_(progress)\n",
    "        # warning for exceeding t_total (only active with warmup_linear\n",
    "        if not nowarn and self.warn_t_total and progress > 1. and progress > self.warned_for_t_total_at_progress:\n",
    "            logger.warning(\n",
    "                \"Training beyond specified 't_total'. Learning rate multiplier set to {}. Please set 't_total' of {} correctly.\"\n",
    "                    .format(ret, self.__class__.__name__))\n",
    "            self.warned_for_t_total_at_progress = progress\n",
    "        # end warning\n",
    "        return ret\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def get_lr_(self, progress):\n",
    "        \"\"\"\n",
    "        :param progress:    value between 0 and 1 (unless going beyond t_total steps) specifying training progress\n",
    "        :return:            learning rate multiplier for current update\n",
    "        \"\"\"\n",
    "        return 1.\n",
    "\n",
    "\n",
    "class ConstantLR(_LRSchedule):\n",
    "    def get_lr_(self, progress):\n",
    "        return 1.\n",
    "\n",
    "\n",
    "class WarmupCosineSchedule(_LRSchedule):\n",
    "    \"\"\"\n",
    "    Linearly increases learning rate from 0 to 1 over `warmup` fraction of training steps.\n",
    "    Decreases learning rate from 1. to 0. over remaining `1 - warmup` steps following a cosine curve.\n",
    "    If `cycles` (default=0.5) is different from default, learning rate follows cosine function after warmup.\n",
    "    \"\"\"\n",
    "    warn_t_total = True\n",
    "    def __init__(self, warmup=0.002, t_total=-1, cycles=.5, **kw):\n",
    "        \"\"\"\n",
    "        :param warmup:      see LRSchedule\n",
    "        :param t_total:     see LRSchedule\n",
    "        :param cycles:      number of cycles. Default: 0.5, corresponding to cosine decay from 1. at progress==warmup and 0 at progress==1.\n",
    "        :param kw:\n",
    "        \"\"\"\n",
    "        super(WarmupCosineSchedule, self).__init__(warmup=warmup, t_total=t_total, **kw)\n",
    "        self.cycles = cycles\n",
    "\n",
    "    def get_lr_(self, progress):\n",
    "        if progress < self.warmup:\n",
    "            return progress / self.warmup\n",
    "        else:\n",
    "            progress = (progress - self.warmup) / (1 - self.warmup)   # progress after warmup\n",
    "            return 0.5 * (1. + math.cos(math.pi * self.cycles * 2 * progress))\n",
    "\n",
    "\n",
    "class WarmupCosineWithHardRestartsSchedule(WarmupCosineSchedule):\n",
    "    \"\"\"\n",
    "    Linearly increases learning rate from 0 to 1 over `warmup` fraction of training steps.\n",
    "    If `cycles` (default=1.) is different from default, learning rate follows `cycles` times a cosine decaying\n",
    "    learning rate (with hard restarts).\n",
    "    \"\"\"\n",
    "    def __init__(self, warmup=0.002, t_total=-1, cycles=1., **kw):\n",
    "        super(WarmupCosineWithHardRestartsSchedule, self).__init__(warmup=warmup, t_total=t_total, cycles=cycles, **kw)\n",
    "        assert(cycles >= 1.)\n",
    "\n",
    "    def get_lr_(self, progress):\n",
    "        if progress < self.warmup:\n",
    "            return progress / self.warmup\n",
    "        else:\n",
    "            progress = (progress - self.warmup) / (1 - self.warmup)     # progress after warmup\n",
    "            ret = 0.5 * (1. + math.cos(math.pi * ((self.cycles * progress) % 1)))\n",
    "            return ret\n",
    "\n",
    "\n",
    "class WarmupCosineWithWarmupRestartsSchedule(WarmupCosineWithHardRestartsSchedule):\n",
    "    \"\"\"\n",
    "    All training progress is divided in `cycles` (default=1.) parts of equal length.\n",
    "    Every part follows a schedule with the first `warmup` fraction of the training steps linearly increasing from 0. to 1.,\n",
    "    followed by a learning rate decreasing from 1. to 0. following a cosine curve.\n",
    "    \"\"\"\n",
    "    def __init__(self, warmup=0.002, t_total=-1, cycles=1., **kw):\n",
    "        assert(warmup * cycles < 1.)\n",
    "        warmup = warmup * cycles if warmup >= 0 else warmup\n",
    "        super(WarmupCosineWithWarmupRestartsSchedule, self).__init__(warmup=warmup, t_total=t_total, cycles=cycles, **kw)\n",
    "\n",
    "    def get_lr_(self, progress):\n",
    "        progress = progress * self.cycles % 1.\n",
    "        if progress < self.warmup:\n",
    "            return progress / self.warmup\n",
    "        else:\n",
    "            progress = (progress - self.warmup) / (1 - self.warmup)     # progress after warmup\n",
    "            ret = 0.5 * (1. + math.cos(math.pi * progress))\n",
    "            return ret\n",
    "\n",
    "\n",
    "class WarmupConstantSchedule(_LRSchedule):\n",
    "    \"\"\"\n",
    "    Linearly increases learning rate from 0 to 1 over `warmup` fraction of training steps.\n",
    "    Keeps learning rate equal to 1. after warmup.\n",
    "    \"\"\"\n",
    "    def get_lr_(self, progress):\n",
    "        if progress < self.warmup:\n",
    "            return progress / self.warmup\n",
    "        return 1.\n",
    "\n",
    "\n",
    "class WarmupLinearSchedule(_LRSchedule):\n",
    "    \"\"\"\n",
    "    Linearly increases learning rate from 0 to 1 over `warmup` fraction of training steps.\n",
    "    Linearly decreases learning rate from 1. to 0. over remaining `1 - warmup` steps.\n",
    "    \"\"\"\n",
    "    warn_t_total = True\n",
    "    def get_lr_(self, progress):\n",
    "        if progress < self.warmup:\n",
    "            return progress / self.warmup\n",
    "        return max((progress - 1.) / (self.warmup - 1.), 0.)\n",
    "\n",
    "\n",
    "SCHEDULES = {\n",
    "    None:       ConstantLR,\n",
    "    \"none\":     ConstantLR,\n",
    "    \"warmup_cosine\": WarmupCosineSchedule,\n",
    "    \"warmup_constant\": WarmupConstantSchedule,\n",
    "    \"warmup_linear\": WarmupLinearSchedule\n",
    "}\n",
    "\n",
    "\n",
    "class BertAdam(Optimizer):\n",
    "    \"\"\"Implements BERT version of Adam algorithm with weight decay fix.\n",
    "    Params:\n",
    "        lr: learning rate\n",
    "        warmup: portion of t_total for the warmup, -1  means no warmup. Default: -1\n",
    "        t_total: total number of training steps for the learning\n",
    "            rate schedule, -1  means constant learning rate of 1. (no warmup regardless of warmup setting). Default: -1\n",
    "        schedule: schedule to use for the warmup (see above).\n",
    "            Can be `'warmup_linear'`, `'warmup_constant'`, `'warmup_cosine'`, `'none'`, `None` or a `_LRSchedule` object (see below).\n",
    "            If `None` or `'none'`, learning rate is always kept constant.\n",
    "            Default : `'warmup_linear'`\n",
    "        b1: Adams b1. Default: 0.9\n",
    "        b2: Adams b2. Default: 0.999\n",
    "        e: Adams epsilon. Default: 1e-6\n",
    "        weight_decay: Weight decay. Default: 0.01\n",
    "        max_grad_norm: Maximum norm for the gradients (-1 means no clipping). Default: 1.0\n",
    "    \"\"\"\n",
    "    def __init__(self, params, lr=required, warmup=-1, t_total=-1, schedule='warmup_linear',\n",
    "                 b1=0.9, b2=0.999, e=1e-6, weight_decay=0.01, max_grad_norm=1.0, **kwargs):\n",
    "        if lr is not required and lr < 0.0:\n",
    "            raise ValueError(\"Invalid learning rate: {} - should be >= 0.0\".format(lr))\n",
    "        if not isinstance(schedule, _LRSchedule) and schedule not in SCHEDULES:\n",
    "            raise ValueError(\"Invalid schedule parameter: {}\".format(schedule))\n",
    "        if not 0.0 <= b1 < 1.0:\n",
    "            raise ValueError(\"Invalid b1 parameter: {} - should be in [0.0, 1.0[\".format(b1))\n",
    "        if not 0.0 <= b2 < 1.0:\n",
    "            raise ValueError(\"Invalid b2 parameter: {} - should be in [0.0, 1.0[\".format(b2))\n",
    "        if not e >= 0.0:\n",
    "            raise ValueError(\"Invalid epsilon value: {} - should be >= 0.0\".format(e))\n",
    "        # initialize schedule object\n",
    "        if not isinstance(schedule, _LRSchedule):\n",
    "            schedule_type = SCHEDULES[schedule]\n",
    "            schedule = schedule_type(warmup=warmup, t_total=t_total)\n",
    "        else:\n",
    "            if warmup != -1 or t_total != -1:\n",
    "                logger.warning(\"warmup and t_total on the optimizer are ineffective when _LRSchedule object is provided as schedule. \"\n",
    "                               \"Please specify custom warmup and t_total in _LRSchedule object.\")\n",
    "        defaults = dict(lr=lr, schedule=schedule,\n",
    "                        b1=b1, b2=b2, e=e, weight_decay=weight_decay,\n",
    "                        max_grad_norm=max_grad_norm)\n",
    "        self.rate = None\n",
    "        super(BertAdam, self).__init__(params, defaults)\n",
    "\n",
    "    def show_lr(self):\n",
    "        return self.rate\n",
    "\n",
    "    def get_lr(self):\n",
    "        lr = []\n",
    "        for group in self.param_groups:\n",
    "            for p in group['params']:\n",
    "                state = self.state[p]\n",
    "                if len(state) == 0:\n",
    "                    return [0]\n",
    "                lr_scheduled = group['lr']\n",
    "                lr_scheduled *= group['schedule'].get_lr(state['step'])\n",
    "                lr.append(lr_scheduled)\n",
    "        return lr\n",
    "\n",
    "    def step(self, closure=None):\n",
    "        \"\"\"Performs a single optimization step.\n",
    "        Arguments:\n",
    "            closure (callable, optional): A closure that reevaluates the model\n",
    "                and returns the loss.\n",
    "        \"\"\"\n",
    "        loss = None\n",
    "        if closure is not None:\n",
    "            loss = closure()\n",
    "\n",
    "        for group in self.param_groups:\n",
    "            for p in group['params']:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                grad = p.grad.data\n",
    "                if grad.is_sparse:\n",
    "                    raise RuntimeError('Adam does not support sparse gradients, please consider SparseAdam instead')\n",
    "\n",
    "                state = self.state[p]\n",
    "\n",
    "                # State initialization\n",
    "                if len(state) == 0:\n",
    "                    state['step'] = 0\n",
    "                    # Exponential moving average of gradient values\n",
    "                    state['next_m'] = torch.zeros_like(p.data)\n",
    "                    # Exponential moving average of squared gradient values\n",
    "                    state['next_v'] = torch.zeros_like(p.data)\n",
    "\n",
    "                next_m, next_v = state['next_m'], state['next_v']\n",
    "                beta1, beta2 = group['b1'], group['b2']\n",
    "\n",
    "                # Add grad clipping\n",
    "                if group['max_grad_norm'] > 0:\n",
    "                    clip_grad_norm_(p, group['max_grad_norm'])\n",
    "\n",
    "                # Decay the first and second moment running average coefficient\n",
    "                # In-place operations to update the averages at the same time\n",
    "                next_m.mul_(beta1).add_(1 - beta1, grad)\n",
    "                next_v.mul_(beta2).addcmul_(1 - beta2, grad, grad)\n",
    "                update = next_m / (next_v.sqrt() + group['e'])\n",
    "\n",
    "                # Just adding the square of the weights to the loss function is *not*\n",
    "                # the correct way of using L2 regularization/weight decay with Adam,\n",
    "                # since that will interact with the m and v parameters in strange ways.\n",
    "                #\n",
    "                # Instead we want to decay the weights in a manner that doesn't interact\n",
    "                # with the m/v parameters. This is equivalent to adding the square\n",
    "                # of the weights to the loss with plain (non-momentum) SGD.\n",
    "                if group['weight_decay'] > 0.0:\n",
    "                    update += group['weight_decay'] * p.data\n",
    "\n",
    "                lr_scheduled = group['lr']\n",
    "                lr_scheduled *= group['schedule'].get_lr(state['step'])\n",
    "\n",
    "                self.rate = lr_scheduled\n",
    "\n",
    "                update_with_lr = lr_scheduled * update\n",
    "                p.data.add_(-update_with_lr)\n",
    "\n",
    "                state['step'] += 1\n",
    "\n",
    "                # step_size = lr_scheduled * math.sqrt(bias_correction2) / bias_correction1\n",
    "                # No bias correction\n",
    "                # bias_correction1 = 1 - beta1 ** state['step']\n",
    "                # bias_correction2 = 1 - beta2 ** state['step']\n",
    "\n",
    "        return loss\n",
    "\n",
    "\n",
    "class Adam(Optimizer):\n",
    "    \"\"\"Implements pytorch version of Adam algorithm with weight decay fix.\n",
    "    Params:\n",
    "        lr: learning rate\n",
    "        warmup: portion of t_total for the warmup, -1  means no warmup. Default: -1\n",
    "        t_total: total number of training steps for the learning\n",
    "            rate schedule, -1  means constant learning rate of 1. (no warmup regardless of warmup setting). Default: -1\n",
    "        schedule: schedule to use for the warmup (see above).\n",
    "            Can be `'warmup_linear'`, `'warmup_constant'`, `'warmup_cosine'`, `'none'`, `None` or a `_LRSchedule` object (see below).\n",
    "            If `None` or `'none'`, learning rate is always kept constant.\n",
    "            Default : `'warmup_linear'`\n",
    "        b1: Adams b1. Default: 0.9\n",
    "        b2: Adams b2. Default: 0.999\n",
    "        e: Adams epsilon. Default: 1e-6\n",
    "        weight_decay: Weight decay. Default: 0.01\n",
    "        max_grad_norm: Maximum norm for the gradients (-1 means no clipping). Default: 1.0\n",
    "    \"\"\"\n",
    "    def __init__(self, params, lr=required, warmup=-1, t_total=-1, schedule='warmup_linear',\n",
    "                 b1=0.9, b2=0.999, e=1e-8, weight_decay=0,  amsgrad=False, max_grad_norm=1.0, **kwargs):\n",
    "        if lr is not required and lr < 0.0:\n",
    "            raise ValueError(\"Invalid learning rate: {} - should be >= 0.0\".format(lr))\n",
    "        if not isinstance(schedule, _LRSchedule) and schedule not in SCHEDULES:\n",
    "            raise ValueError(\"Invalid schedule parameter: {}\".format(schedule))\n",
    "        if not 0.0 <= b1 < 1.0:\n",
    "            raise ValueError(\"Invalid b1 parameter: {} - should be in [0.0, 1.0[\".format(b1))\n",
    "        if not 0.0 <= b2 < 1.0:\n",
    "            raise ValueError(\"Invalid b2 parameter: {} - should be in [0.0, 1.0[\".format(b2))\n",
    "        if not e >= 0.0:\n",
    "            raise ValueError(\"Invalid epsilon value: {} - should be >= 0.0\".format(e))\n",
    "        # initialize schedule object\n",
    "        if not isinstance(schedule, _LRSchedule):\n",
    "            schedule_type = SCHEDULES[schedule]\n",
    "            schedule = schedule_type(warmup=warmup, t_total=t_total)\n",
    "        else:\n",
    "            if warmup != -1 or t_total != -1:\n",
    "                logger.warning(\"warmup and t_total on the optimizer are ineffective when _LRSchedule object is provided as schedule. \"\n",
    "                               \"Please specify custom warmup and t_total in _LRSchedule object.\")\n",
    "        defaults = dict(lr=lr, schedule=schedule,\n",
    "                        b1=b1, b2=b2, e=e, weight_decay=weight_decay,\n",
    "                        amsgrad=amsgrad, max_grad_norm=max_grad_norm)\n",
    "        self.rate = None\n",
    "        super(Adam, self).__init__(params, defaults)\n",
    "\n",
    "    def __setstate__(self, state):\n",
    "        super(Adam, self).__setstate__(state)\n",
    "        for group in self.param_groups:\n",
    "            group.setdefault('amsgrad', False)\n",
    "\n",
    "    def show_lr(self):\n",
    "        return self.rate\n",
    "\n",
    "    def get_lr(self):\n",
    "        lr = []\n",
    "        for group in self.param_groups:\n",
    "            for p in group['params']:\n",
    "                state = self.state[p]\n",
    "                if len(state) == 0:\n",
    "                    return [0]\n",
    "                lr_scheduled = group['lr']\n",
    "                lr_scheduled *= group['schedule'].get_lr(state['step'])\n",
    "                lr.append(lr_scheduled)\n",
    "        return lr\n",
    "\n",
    "    def step(self, closure=None):\n",
    "        \"\"\"Performs a single optimization step.\n",
    "        Arguments:\n",
    "            closure (callable, optional): A closure that reevaluates the model\n",
    "                and returns the loss.\n",
    "        \"\"\"\n",
    "        loss = None\n",
    "        if closure is not None:\n",
    "            loss = closure()\n",
    "\n",
    "        for group in self.param_groups:\n",
    "            for p in group['params']:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                grad = p.grad.data\n",
    "                if grad.is_sparse:\n",
    "                    raise RuntimeError('Adam does not support sparse gradients, please consider SparseAdam instead')\n",
    "                amsgrad = group['amsgrad']\n",
    "\n",
    "                state = self.state[p]\n",
    "\n",
    "                # State initialization\n",
    "                if len(state) == 0:\n",
    "                    state['step'] = 0\n",
    "                    # Exponential moving average of gradient values\n",
    "                    state['exp_avg'] = torch.zeros_like(p.data)\n",
    "                    # Exponential moving average of squared gradient values\n",
    "                    state['exp_avg_sq'] = torch.zeros_like(p.data)\n",
    "                    if amsgrad:\n",
    "                        # Maintains max of all exp. moving avg. of sq. grad. values\n",
    "                        state['max_exp_avg_sq'] = torch.zeros_like(p.data)\n",
    "\n",
    "                exp_avg, exp_avg_sq = state['exp_avg'], state['exp_avg_sq']\n",
    "                if amsgrad:\n",
    "                    max_exp_avg_sq = state['max_exp_avg_sq']\n",
    "                beta1, beta2 = group['b1'], group['b2']\n",
    "\n",
    "                # Add grad clipping\n",
    "                if group['max_grad_norm'] > 0:\n",
    "                    clip_grad_norm_(p, group['max_grad_norm'])\n",
    "\n",
    "                lr_scheduled = group['lr']\n",
    "                lr_scheduled *= group['schedule'].get_lr(state['step'])\n",
    "                self.rate = lr_scheduled\n",
    "\n",
    "                state['step'] += 1\n",
    "\n",
    "                if group['weight_decay'] != 0:\n",
    "                    grad.add_(group['weight_decay'], p.data)\n",
    "\n",
    "                # Decay the first and second moment running average coefficient\n",
    "                exp_avg.mul_(beta1).add_(1 - beta1, grad)\n",
    "                exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\n",
    "                if amsgrad:\n",
    "                    # Maintains the maximum of all 2nd moment running avg. till now\n",
    "                    torch.max(max_exp_avg_sq, exp_avg_sq, out=max_exp_avg_sq)\n",
    "                    # Use the max. for normalizing running avg. of gradient\n",
    "                    denom = max_exp_avg_sq.sqrt().add_(group['e'])\n",
    "                else:\n",
    "                    denom = exp_avg_sq.sqrt().add_(group['e'])\n",
    "\n",
    "                bias_correction1 = 1 - beta1 ** state['step']\n",
    "                bias_correction2 = 1 - beta2 ** state['step']\n",
    "                step_size = lr_scheduled * math.sqrt(bias_correction2) / bias_correction1\n",
    "\n",
    "                p.data.addcdiv_(-step_size, exp_avg, denom)\n",
    "\n",
    "        return loss\n",
    "\n",
    "class Adamax(Optimizer):\n",
    "    \"\"\"Implements Adamax algorithm (a variant of Adam based on infinity norm).\n",
    "    It has been proposed in `Adam: A Method for Stochastic Optimization`__.\n",
    "    Arguments:\n",
    "        params (iterable): iterable of parameters to optimize or dicts defining\n",
    "            parameter groups\n",
    "        lr (float, optional): learning rate (default: 2e-3)\n",
    "        betas (Tuple[float, float], optional): coefficients used for computing\n",
    "            running averages of gradient and its square\n",
    "        eps (float, optional): term added to the denominator to improve\n",
    "            numerical stability (default: 1e-8)\n",
    "        weight_decay (float, optional): weight decay (L2 penalty) (default: 0)\n",
    "    __ https://arxiv.org/abs/1412.6980\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, params, lr=required, warmup=-1, t_total=-1, schedule='warmup_linear',\n",
    "                 b1=0.9, b2=0.999, e=1e-8, weight_decay=0, max_grad_norm=1.0, **kwargs):\n",
    "        if not 0.0 <= lr:\n",
    "            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n",
    "        if not 0.0 <= e:\n",
    "            raise ValueError(\"Invalid epsilon value: {}\".format(e))\n",
    "        if not 0.0 <= b1 < 1.0:\n",
    "            raise ValueError(\"Invalid b1 parameter: {} - should be in [0.0, 1.0[\".format(b1))\n",
    "        if not 0.0 <= b2 < 1.0:\n",
    "            raise ValueError(\"Invalid b2 parameter: {} - should be in [0.0, 1.0[\".format(b2))\n",
    "        if not 0.0 <= weight_decay:\n",
    "            raise ValueError(\"Invalid weight_decay value: {}\".format(weight_decay))\n",
    "\n",
    "        if not isinstance(schedule, _LRSchedule):\n",
    "            schedule_type = SCHEDULES[schedule]            \n",
    "            schedule = schedule_type(warmup=warmup, t_total=t_total)\n",
    "        else:\n",
    "            if warmup != -1 or t_total != -1:\n",
    "                logger.warning(\"warmup and t_total on the optimizer are ineffective when _LRSchedule object is provided as schedule. \"\n",
    "                               \"Please specify custom warmup and t_total in _LRSchedule object.\")\n",
    "\n",
    "        defaults = dict(lr=lr, schedule=schedule,\n",
    "                        b1=b1, b2=b2, e=e, weight_decay=weight_decay, max_grad_norm=max_grad_norm)\n",
    "        self.rate = None\n",
    "        super(Adamax, self).__init__(params, defaults)\n",
    "\n",
    "    def show_lr(self):\n",
    "        return self.rate\n",
    "\n",
    "    def get_lr(self):\n",
    "        lr = []\n",
    "        for group in self.param_groups:\n",
    "            for p in group['params']:\n",
    "                state = self.state[p]\n",
    "                if len(state) == 0:\n",
    "                    return [0]\n",
    "                lr_scheduled = group['lr']\n",
    "                lr_scheduled *= group['schedule'].get_lr(state['step'])\n",
    "                lr.append(lr_scheduled)\n",
    "        return lr\n",
    "\n",
    "    def step(self, closure=None):\n",
    "        \"\"\"Performs a single optimization step.\n",
    "        Arguments:\n",
    "            closure (callable, optional): A closure that reevaluates the model\n",
    "                and returns the loss.\n",
    "        \"\"\"\n",
    "        loss = None\n",
    "        if closure is not None:\n",
    "            loss = closure()\n",
    "\n",
    "        for group in self.param_groups:\n",
    "            for p in group['params']:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                grad = p.grad.data\n",
    "                if grad.is_sparse:\n",
    "                    raise RuntimeError('Adamax does not support sparse gradients')\n",
    "                state = self.state[p]\n",
    "\n",
    "                # State initialization\n",
    "                if len(state) == 0:\n",
    "                    state['step'] = 0\n",
    "                    state['exp_avg'] = torch.zeros_like(p.data)\n",
    "                    state['exp_inf'] = torch.zeros_like(p.data)\n",
    "\n",
    "                exp_avg, exp_inf = state['exp_avg'], state['exp_inf']\n",
    "                beta1, beta2 = group['b1'], group['b2']\n",
    "                eps = group['e']\n",
    "\n",
    "                # Add grad clipping\n",
    "                if group['max_grad_norm'] > 0:\n",
    "                    clip_grad_norm_(p, group['max_grad_norm'])\n",
    "\n",
    "                lr_scheduled = group['lr']\n",
    "                lr_scheduled *= group['schedule'].get_lr(state['step'])\n",
    "\n",
    "                self.rate = lr_scheduled\n",
    "                state['step'] += 1\n",
    "\n",
    "                if group['weight_decay'] != 0:\n",
    "                    grad = grad.add(group['weight_decay'], p.data)\n",
    "\n",
    "                # Update biased first moment estimate.\n",
    "                exp_avg.mul_(beta1).add_(1 - beta1, grad)\n",
    "                # Update the exponentially weighted infinity norm.\n",
    "                norm_buf = torch.cat([\n",
    "                    exp_inf.mul_(beta2).unsqueeze(0),\n",
    "                    grad.abs().add_(eps).unsqueeze_(0)\n",
    "                ], 0)\n",
    "                torch.max(norm_buf, 0, keepdim=False, out=(exp_inf, exp_inf.new().long()))\n",
    "\n",
    "                bias_correction = 1 - beta1 ** state['step']\n",
    "                clr = lr_scheduled / bias_correction\n",
    "\n",
    "                p.data.addcdiv_(-clr, exp_avg, exp_inf)\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "tokenizer = T5Tokenizer.from_pretrained('t5-small')\n",
    "model = T5ForConditionalGeneration.from_pretrained('t5-small')\n",
    "model = model.to(device)\n",
    "# optimizer should be defined outside training loop\n",
    "# optimizer = optim.SGD(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epoch = 50\n",
    "no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n",
    "optimizer_grouped_parameters = []\n",
    "lr=2e-5\n",
    "for key, value in dict(model.named_parameters()).items():\n",
    "    if value.requires_grad:\n",
    "        if any(nd in key for nd in no_decay):\n",
    "            optimizer_grouped_parameters += [\n",
    "                {\"params\": [value], \"lr\": lr, \"weight_decay\": 0.01}\n",
    "            ]\n",
    "        if not any(nd in key for nd in no_decay):\n",
    "            optimizer_grouped_parameters += [\n",
    "                {\"params\": [value], \"lr\": lr, \"weight_decay\": 0.0}\n",
    "            ]\n",
    "                \n",
    "optimizer = BertAdam(\n",
    "            optimizer_grouped_parameters,\n",
    "            lr=lr,\n",
    "            warmup=0.1,\n",
    "            t_total=100,\n",
    "#             t_total=num_epoch * len(training_generator),\n",
    "            schedule='warmup_constant',\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "6zQaAWNlcBK-",
    "outputId": "3633c9d3-d2ac-4111-d632-8cb6b897b767"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch  0  completed!, Training LOSS is:  tensor(3.2990, device='cuda:0', grad_fn=<NllLossBackward>)  Validation loss is:  tensor(33.8064, device='cuda:0')\n",
      "\n",
      "Epoch  1  completed!, Training LOSS is:  tensor(4.0996, device='cuda:0', grad_fn=<NllLossBackward>)  Validation loss is:  tensor(38.8854, device='cuda:0')\n",
      "\n",
      "Epoch  2  completed!, Training LOSS is:  tensor(2.8176, device='cuda:0', grad_fn=<NllLossBackward>)  Validation loss is:  tensor(13.5292, device='cuda:0')\n",
      "\n",
      "Epoch  3  completed!, Training LOSS is:  tensor(3.5123, device='cuda:0', grad_fn=<NllLossBackward>)  Validation loss is:  tensor(18.1840, device='cuda:0')\n",
      "\n",
      "Epoch  4  completed!, Training LOSS is:  tensor(3.5333, device='cuda:0', grad_fn=<NllLossBackward>)  Validation loss is:  tensor(32.9379, device='cuda:0')\n",
      "\n",
      "Epoch  5  completed!, Training LOSS is:  tensor(2.5911, device='cuda:0', grad_fn=<NllLossBackward>)  Validation loss is:  tensor(18.0464, device='cuda:0')\n",
      "\n",
      "Epoch  6  completed!, Training LOSS is:  tensor(2.6745, device='cuda:0', grad_fn=<NllLossBackward>)  Validation loss is:  tensor(35.8887, device='cuda:0')\n",
      "\n",
      "Epoch  7  completed!, Training LOSS is:  tensor(2.6014, device='cuda:0', grad_fn=<NllLossBackward>)  Validation loss is:  tensor(22.7300, device='cuda:0')\n",
      "\n",
      "Epoch  8  completed!, Training LOSS is:  tensor(3.0323, device='cuda:0', grad_fn=<NllLossBackward>)  Validation loss is:  tensor(27.4018, device='cuda:0')\n",
      "\n",
      "Epoch  9  completed!, Training LOSS is:  tensor(3.2175, device='cuda:0', grad_fn=<NllLossBackward>)  Validation loss is:  tensor(26.7457, device='cuda:0')\n",
      "\n",
      "Epoch  10  completed!, Training LOSS is:  tensor(3.2945, device='cuda:0', grad_fn=<NllLossBackward>)  Validation loss is:  tensor(35.1803, device='cuda:0')\n",
      "\n",
      "Epoch  11  completed!, Training LOSS is:  tensor(2.5336, device='cuda:0', grad_fn=<NllLossBackward>)  Validation loss is:  tensor(26.8479, device='cuda:0')\n",
      "\n",
      "Epoch  12  completed!, Training LOSS is:  tensor(3.5238, device='cuda:0', grad_fn=<NllLossBackward>)  Validation loss is:  tensor(32.3007, device='cuda:0')\n",
      "\n",
      "Epoch  13  completed!, Training LOSS is:  tensor(2.4630, device='cuda:0', grad_fn=<NllLossBackward>)  Validation loss is:  tensor(39.4667, device='cuda:0')\n",
      "\n",
      "Epoch  14  completed!, Training LOSS is:  tensor(3.5892, device='cuda:0', grad_fn=<NllLossBackward>)  Validation loss is:  tensor(24.9855, device='cuda:0')\n",
      "\n",
      "Epoch  15  completed!, Training LOSS is:  tensor(2.5773, device='cuda:0', grad_fn=<NllLossBackward>)  Validation loss is:  tensor(26.4958, device='cuda:0')\n",
      "\n",
      "Epoch  16  completed!, Training LOSS is:  tensor(2.1221, device='cuda:0', grad_fn=<NllLossBackward>)  Validation loss is:  tensor(48.8534, device='cuda:0')\n",
      "\n",
      "Epoch  17  completed!, Training LOSS is:  tensor(3.3648, device='cuda:0', grad_fn=<NllLossBackward>)  Validation loss is:  tensor(40.7692, device='cuda:0')\n",
      "\n",
      "Epoch  18  completed!, Training LOSS is:  tensor(2.6696, device='cuda:0', grad_fn=<NllLossBackward>)  Validation loss is:  tensor(35.9252, device='cuda:0')\n",
      "\n",
      "Epoch  19  completed!, Training LOSS is:  tensor(2.3081, device='cuda:0', grad_fn=<NllLossBackward>)  Validation loss is:  tensor(30.9514, device='cuda:0')\n",
      "\n",
      "Epoch  20  completed!, Training LOSS is:  tensor(1.6755, device='cuda:0', grad_fn=<NllLossBackward>)  Validation loss is:  tensor(30.9932, device='cuda:0')\n",
      "\n",
      "Epoch  21  completed!, Training LOSS is:  tensor(2.9550, device='cuda:0', grad_fn=<NllLossBackward>)  Validation loss is:  tensor(47.9949, device='cuda:0')\n",
      "\n",
      "Epoch  22  completed!, Training LOSS is:  tensor(3.1115, device='cuda:0', grad_fn=<NllLossBackward>)  Validation loss is:  tensor(60.2404, device='cuda:0')\n",
      "\n",
      "Epoch  23  completed!, Training LOSS is:  tensor(2.0752, device='cuda:0', grad_fn=<NllLossBackward>)  Validation loss is:  tensor(24.2179, device='cuda:0')\n",
      "\n",
      "Epoch  24  completed!, Training LOSS is:  tensor(2.6061, device='cuda:0', grad_fn=<NllLossBackward>)  Validation loss is:  tensor(37.5002, device='cuda:0')\n",
      "\n",
      "Epoch  25  completed!, Training LOSS is:  tensor(2.0007, device='cuda:0', grad_fn=<NllLossBackward>)  Validation loss is:  tensor(41.3933, device='cuda:0')\n",
      "\n",
      "Epoch  26  completed!, Training LOSS is:  tensor(2.6643, device='cuda:0', grad_fn=<NllLossBackward>)  Validation loss is:  tensor(61.8447, device='cuda:0')\n",
      "\n",
      "Epoch  27  completed!, Training LOSS is:  tensor(2.1630, device='cuda:0', grad_fn=<NllLossBackward>)  Validation loss is:  tensor(47.1583, device='cuda:0')\n",
      "\n",
      "Epoch  28  completed!, Training LOSS is:  tensor(3.1927, device='cuda:0', grad_fn=<NllLossBackward>)  Validation loss is:  tensor(49.6087, device='cuda:0')\n",
      "\n",
      "Epoch  29  completed!, Training LOSS is:  tensor(2.5440, device='cuda:0', grad_fn=<NllLossBackward>)  Validation loss is:  tensor(46.4841, device='cuda:0')\n",
      "\n",
      "Epoch  30  completed!, Training LOSS is:  tensor(2.1557, device='cuda:0', grad_fn=<NllLossBackward>)  Validation loss is:  tensor(53.1847, device='cuda:0')\n",
      "\n",
      "Epoch  31  completed!, Training LOSS is:  tensor(2.3262, device='cuda:0', grad_fn=<NllLossBackward>)  Validation loss is:  tensor(57.8831, device='cuda:0')\n",
      "\n",
      "Epoch  32  completed!, Training LOSS is:  tensor(2.5160, device='cuda:0', grad_fn=<NllLossBackward>)  Validation loss is:  tensor(59.6778, device='cuda:0')\n",
      "\n",
      "Epoch  33  completed!, Training LOSS is:  tensor(1.7574, device='cuda:0', grad_fn=<NllLossBackward>)  Validation loss is:  tensor(53.0482, device='cuda:0')\n",
      "\n",
      "Epoch  34  completed!, Training LOSS is:  tensor(2.2347, device='cuda:0', grad_fn=<NllLossBackward>)  Validation loss is:  tensor(56.4339, device='cuda:0')\n",
      "\n",
      "Epoch  35  completed!, Training LOSS is:  tensor(2.3974, device='cuda:0', grad_fn=<NllLossBackward>)  Validation loss is:  tensor(44.5233, device='cuda:0')\n",
      "\n",
      "Epoch  36  completed!, Training LOSS is:  tensor(2.0578, device='cuda:0', grad_fn=<NllLossBackward>)  Validation loss is:  tensor(54.4339, device='cuda:0')\n",
      "\n",
      "Epoch  37  completed!, Training LOSS is:  tensor(1.7530, device='cuda:0', grad_fn=<NllLossBackward>)  Validation loss is:  tensor(59.8830, device='cuda:0')\n",
      "\n",
      "Epoch  38  completed!, Training LOSS is:  tensor(1.9004, device='cuda:0', grad_fn=<NllLossBackward>)  Validation loss is:  tensor(45.2562, device='cuda:0')\n",
      "\n",
      "Epoch  39  completed!, Training LOSS is:  tensor(2.2293, device='cuda:0', grad_fn=<NllLossBackward>)  Validation loss is:  tensor(46.8483, device='cuda:0')\n",
      "\n",
      "Epoch  40  completed!, Training LOSS is:  tensor(1.7948, device='cuda:0', grad_fn=<NllLossBackward>)  Validation loss is:  tensor(53.4801, device='cuda:0')\n",
      "\n",
      "Epoch  41  completed!, Training LOSS is:  tensor(2.2564, device='cuda:0', grad_fn=<NllLossBackward>)  Validation loss is:  tensor(45.6873, device='cuda:0')\n",
      "\n",
      "Epoch  42  completed!, Training LOSS is:  tensor(1.4762, device='cuda:0', grad_fn=<NllLossBackward>)  Validation loss is:  tensor(57.5059, device='cuda:0')\n",
      "\n",
      "Epoch  43  completed!, Training LOSS is:  tensor(1.8475, device='cuda:0', grad_fn=<NllLossBackward>)  Validation loss is:  tensor(50.1445, device='cuda:0')\n",
      "\n",
      "Epoch  44  completed!, Training LOSS is:  tensor(2.1584, device='cuda:0', grad_fn=<NllLossBackward>)  Validation loss is:  tensor(56.4393, device='cuda:0')\n",
      "\n",
      "Epoch  45  completed!, Training LOSS is:  tensor(1.7562, device='cuda:0', grad_fn=<NllLossBackward>)  Validation loss is:  tensor(49.6157, device='cuda:0')\n",
      "\n",
      "Epoch  46  completed!, Training LOSS is:  tensor(3.2476, device='cuda:0', grad_fn=<NllLossBackward>)  Validation loss is:  tensor(11.0171, device='cuda:0')\n",
      "\n",
      "Epoch  47  completed!, Training LOSS is:  tensor(2.0269, device='cuda:0', grad_fn=<NllLossBackward>)  Validation loss is:  tensor(28.3715, device='cuda:0')\n",
      "\n",
      "Epoch  48  completed!, Training LOSS is:  tensor(1.7869, device='cuda:0', grad_fn=<NllLossBackward>)  Validation loss is:  tensor(34.9047, device='cuda:0')\n",
      "\n",
      "Epoch  49  completed!, Training LOSS is:  tensor(1.3926, device='cuda:0', grad_fn=<NllLossBackward>)  Validation loss is:  tensor(51.6380, device='cuda:0')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fb759af7908>]"
      ]
     },
     "execution_count": 56,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "loss_train = []\n",
    "loss_valid = []\n",
    "\n",
    "for epoch in range(num_epoch):\n",
    "    trainloss=0\n",
    "    validloss=0\n",
    "    # Training\n",
    "    for local_batch, local_labels in training_generator: \n",
    "        model.zero_grad()\n",
    "        \"\"\"Forward Function Implementation\"\"\"\n",
    "        # Also, put the encode function in dataset. Return encoded matrix values from dataset. Here, only take it to gpu.\n",
    "        input_ids = tokenizer.batch_encode_plus(local_batch, return_tensors=\"pt\",pad_to_max_length=True)\n",
    "        label = tokenizer.batch_encode_plus(local_labels, return_tensors=\"pt\",pad_to_max_length=True)\n",
    "        outputs = model(input_ids=(input_ids['input_ids']).to(device), lm_labels=(label['input_ids']).to(device),attention_mask=(input_ids['attention_mask']).to(device))\n",
    "        loss, prediction_scores = outputs[:2]\n",
    "        trainloss=loss\n",
    "        \"\"\"Forward Function Ends here\"\"\"\n",
    "        \"\"\"Loss and optimizer\"\"\"\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    loss_train.append(trainloss)\n",
    "\n",
    "\n",
    "    \"\"\"Validation\"\"\"\n",
    "    with torch.set_grad_enabled(False):\n",
    "        for local_batch, local_labels in validation_generator:\n",
    "          input_ids = tokenizer.batch_encode_plus(local_batch, return_tensors=\"pt\",pad_to_max_length=True)\n",
    "          label = tokenizer.batch_encode_plus(local_labels, return_tensors=\"pt\",pad_to_max_length=True)\n",
    "          model.eval()\n",
    "          outputs = model(input_ids=(input_ids['input_ids']).to(device), lm_labels=(label['input_ids']).to(device))\n",
    "          loss, prediction_scores = outputs[:2]\n",
    "          validloss=loss\n",
    "    loss_valid.append(validloss)\n",
    "    print(\"\\nEpoch \", epoch, \" completed!, Training LOSS is: \", trainloss, \" Validation loss is: \", validloss)\n",
    "plt.plot(loss_train)\n",
    "plt.plot(loss_valid)\n",
    "    #print(loss, \"\\n\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "colab_type": "code",
    "id": "1Q5Y0eA2_bnH",
    "outputId": "9ad413b6-d225-40f1-8915-a687ac2882b4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
      "\n",
      "Enter your authorization code:\n",
      "··········\n",
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VMdN-HAVDuGm"
   },
   "outputs": [],
   "source": [
    " model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "id": "pVOZGflPBMIk",
    "outputId": "a953e9a2-68e9-48b4-a740-60782d11a4a3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creates an instance of the factory.\n",
      "( ConstructorDeclaration ( StatementExpression ( SuperConstructorInvocation ) SuperConstructorInvocation ) StatementExpression ) ConstructorDeclaration\n",
      "\n",
      "creates an instance of the factory .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "temp_index=50 \n",
    "ip=tokenizer.encode(X[temp_index], return_tensors=\"pt\")\n",
    "op=model.generate(input_ids=ip.to(device))\n",
    "print(tokenizer.decode(op[0]))\n",
    "print(X[temp_index])\n",
    "print(Y[temp_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oXLsw4qsNmPq"
   },
   "outputs": [],
   "source": [
    "from datetime import datetime as dt\n",
    "runtime =  dt.now()\n",
    "model_name = \"/content/drive/My Drive/Transformer/models/\" + runtime.strftime(\"%d:%m:%Y:%H:%M\") + \".pth\"\n",
    "\n",
    "torch.save(model.state_dict(), model_name)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "firstTrial.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
