{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "firstTrial.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.2"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "c06fa29ce6f341338b8d59ec1436a874": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_aad42e1fd8d14b80960e754091958526",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_433508b5bbc54ddd95c33ad17351877f",
              "IPY_MODEL_c784e079f0d24752b5faa3dabe8d9863"
            ]
          }
        },
        "aad42e1fd8d14b80960e754091958526": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "433508b5bbc54ddd95c33ad17351877f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "IntProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_7e023fef2d9047d58e8a83d4f549949d",
            "_dom_classes": [],
            "description": " 24%",
            "_model_name": "IntProgressModel",
            "bar_style": "danger",
            "max": 89163,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 21386,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_e59b2e875a7640c2a250b2877bdbcb5e"
          }
        },
        "c784e079f0d24752b5faa3dabe8d9863": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_77244fe9ab54444b86812fec92ea5890",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 21386/89163 [2:40:17&lt;8:05:45,  2.33it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_426faa8edb8246429a4f93bd89e44df0"
          }
        },
        "7e023fef2d9047d58e8a83d4f549949d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "e59b2e875a7640c2a250b2877bdbcb5e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "77244fe9ab54444b86812fec92ea5890": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "426faa8edb8246429a4f93bd89e44df0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/codekali/Neural-SC-Descriptor/blob/master/firstTrial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "1Q5Y0eA2_bnH",
        "outputId": "3af20f55-600c-4678-a861-758eea317beb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Gcmj8Cb9a4aN",
        "outputId": "a2f593ce-ec5e-454d-946e-d4e694489c88",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "import torch\n",
        "import json\n",
        "from tqdm.notebook import tqdm\n",
        "from torch.utils.data import Dataset\n",
        "dictionary={}\n",
        "X,Y = [],[]\n",
        "count=-1   \n",
        "file=open(\"/content/drive/My Drive/Transformer/train/train.token.sbt\")\n",
        "for line in file:\n",
        "  X.append(line)\n",
        "  count+=1\n",
        "count=0\n",
        "file=open(\"/content/drive/My Drive/Transformer/train/train.token.nl\")\n",
        "for line in file:\n",
        "  Y.append(line)\n",
        "  count+=1\n",
        "print(len(Y))\n",
        "print(len(X))\n",
        "#X=X[0:100]\n",
        "#Y=Y[0:100]"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "445812\n",
            "445812\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "LSCj9GE0EDLy",
        "outputId": "d3116573-3106-4e62-c6db-977e237e98d8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# You can even put this data load operation in your Dataset class\n",
        "import torch\n",
        "import json\n",
        "from torch.utils.data import Dataset\n",
        "dictionary={}\n",
        "X_valid,Y_valid = [],[]\n",
        "count=-1   \n",
        "file=open(\"/content/drive/My Drive/Transformer/val/valid.token.sbt\")\n",
        "for line in file:\n",
        "  X_valid.append(line)\n",
        "  count+=1\n",
        "count=0\n",
        "file=open(\"/content/drive/My Drive/Transformer/val/valid.token.nl\")\n",
        "for line in file:\n",
        "  Y_valid.append(line)\n",
        "  count+=1\n",
        "print(len(Y_valid))\n",
        "print(len(X_valid))\n",
        "#X_valid=X_valid[0:20]\n",
        "#Y_valid=Y_valid[0:20]"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "20000\n",
            "20000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "E65v5JOxdj99",
        "outputId": "999633c1-b043-42f9-9f2a-ec752c647f95",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "print(len(Y))\n",
        "print(len(X))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "445812\n",
            "445812\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "2swvO7qqbAb6",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "from torch.utils import data\n",
        "\n",
        "class Dataset(data.Dataset):\n",
        "  def __init__(self, X_item, Y_item):\n",
        "    self.X_item=X_item\n",
        "    self.Y_item=Y_item\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.X_item)\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    X = self.X_item[index]\n",
        "    \n",
        "    Y = self.Y_item[index]\n",
        "    return X, Y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "4jFm-_ywbY7z",
        "outputId": "0d446718-22f2-480f-efea-3253217e51e1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#import mydata\n",
        "import torch\n",
        "from torch.utils import data\n",
        "use_cuda = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "print(device)\n",
        "params = {'batch_size': 5,\n",
        "          'shuffle': True,\n",
        "          'num_workers': 0}\n",
        "#max_epochs = 10\n",
        "\n",
        "training_set = Dataset(X,Y)\n",
        "training_generator = data.DataLoader(training_set, **params)\n",
        "\n",
        "validation_set = Dataset(X_valid,Y_valid)\n",
        "validation_generator = data.DataLoader(validation_set, **params)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "7LgliBxibZKp",
        "scrolled": true,
        "outputId": "ad4993c5-e532-4252-e8b0-36f2401d5d13",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 428
        }
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (2.8.0)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (from transformers) (0.1.86)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.38.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: tokenizers==0.5.2 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.5.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.21.0)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.41)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from transformers) (1.12.46)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.8)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.4.5.1)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.12.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.14.1)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.9.5)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.3.3)\n",
            "Requirement already satisfied: botocore<1.16.0,>=1.15.46 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (1.15.46)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.16.0,>=1.15.46->boto3->transformers) (0.15.2)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.16.0,>=1.15.46->boto3->transformers) (2.8.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5B1p1kvrJGM3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# coding=utf-8\n",
        "# Copyright 2018 The Google AI Language Team Authors and The HuggingFace Inc. team.\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "\"\"\"PyTorch optimization for BERT model.\"\"\"\n",
        "\n",
        "import math\n",
        "import torch\n",
        "from torch.optim import Optimizer\n",
        "from torch.optim.optimizer import required\n",
        "from torch.nn.utils import clip_grad_norm_\n",
        "import logging\n",
        "import abc\n",
        "import sys\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "\n",
        "if sys.version_info >= (3, 4):\n",
        "    ABC = abc.ABC\n",
        "else:\n",
        "    ABC = abc.ABCMeta('ABC', (), {})\n",
        "\n",
        "\n",
        "class _LRSchedule(ABC):\n",
        "    \"\"\" Parent of all LRSchedules here. \"\"\"\n",
        "    warn_t_total = False        # is set to True for schedules where progressing beyond t_total steps doesn't make sense\n",
        "    def __init__(self, warmup=0.002, t_total=-1, **kw):\n",
        "        \"\"\"\n",
        "        :param warmup:  what fraction of t_total steps will be used for linear warmup\n",
        "        :param t_total: how many training steps (updates) are planned\n",
        "        :param kw:\n",
        "        \"\"\"\n",
        "        super(_LRSchedule, self).__init__(**kw)\n",
        "        if t_total < 0:\n",
        "            logger.warning(\"t_total value of {} results in schedule not being applied\".format(t_total))\n",
        "        if not 0.0 <= warmup < 1.0 and not warmup == -1:\n",
        "            raise ValueError(\"Invalid warmup: {} - should be in [0.0, 1.0[ or -1\".format(warmup))\n",
        "        warmup = max(warmup, 0.)\n",
        "        self.warmup, self.t_total = float(warmup), float(t_total)\n",
        "        self.warned_for_t_total_at_progress = -1\n",
        "\n",
        "    def get_lr(self, step, nowarn=False):\n",
        "        \"\"\"\n",
        "        :param step:    which of t_total steps we're on\n",
        "        :param nowarn:  set to True to suppress warning regarding training beyond specified 't_total' steps\n",
        "        :return:        learning rate multiplier for current update\n",
        "        \"\"\"\n",
        "        if self.t_total < 0:\n",
        "            return 1.\n",
        "        progress = float(step) / self.t_total\n",
        "        ret = self.get_lr_(progress)\n",
        "        # warning for exceeding t_total (only active with warmup_linear\n",
        "        if not nowarn and self.warn_t_total and progress > 1. and progress > self.warned_for_t_total_at_progress:\n",
        "            logger.warning(\n",
        "                \"Training beyond specified 't_total'. Learning rate multiplier set to {}. Please set 't_total' of {} correctly.\"\n",
        "                    .format(ret, self.__class__.__name__))\n",
        "            self.warned_for_t_total_at_progress = progress\n",
        "        # end warning\n",
        "        return ret\n",
        "\n",
        "    @abc.abstractmethod\n",
        "    def get_lr_(self, progress):\n",
        "        \"\"\"\n",
        "        :param progress:    value between 0 and 1 (unless going beyond t_total steps) specifying training progress\n",
        "        :return:            learning rate multiplier for current update\n",
        "        \"\"\"\n",
        "        return 1.\n",
        "\n",
        "\n",
        "class ConstantLR(_LRSchedule):\n",
        "    def get_lr_(self, progress):\n",
        "        return 1.\n",
        "\n",
        "\n",
        "class WarmupCosineSchedule(_LRSchedule):\n",
        "    \"\"\"\n",
        "    Linearly increases learning rate from 0 to 1 over `warmup` fraction of training steps.\n",
        "    Decreases learning rate from 1. to 0. over remaining `1 - warmup` steps following a cosine curve.\n",
        "    If `cycles` (default=0.5) is different from default, learning rate follows cosine function after warmup.\n",
        "    \"\"\"\n",
        "    warn_t_total = True\n",
        "    def __init__(self, warmup=0.002, t_total=-1, cycles=.5, **kw):\n",
        "        \"\"\"\n",
        "        :param warmup:      see LRSchedule\n",
        "        :param t_total:     see LRSchedule\n",
        "        :param cycles:      number of cycles. Default: 0.5, corresponding to cosine decay from 1. at progress==warmup and 0 at progress==1.\n",
        "        :param kw:\n",
        "        \"\"\"\n",
        "        super(WarmupCosineSchedule, self).__init__(warmup=warmup, t_total=t_total, **kw)\n",
        "        self.cycles = cycles\n",
        "\n",
        "    def get_lr_(self, progress):\n",
        "        if progress < self.warmup:\n",
        "            return progress / self.warmup\n",
        "        else:\n",
        "            progress = (progress - self.warmup) / (1 - self.warmup)   # progress after warmup\n",
        "            return 0.5 * (1. + math.cos(math.pi * self.cycles * 2 * progress))\n",
        "\n",
        "\n",
        "class WarmupCosineWithHardRestartsSchedule(WarmupCosineSchedule):\n",
        "    \"\"\"\n",
        "    Linearly increases learning rate from 0 to 1 over `warmup` fraction of training steps.\n",
        "    If `cycles` (default=1.) is different from default, learning rate follows `cycles` times a cosine decaying\n",
        "    learning rate (with hard restarts).\n",
        "    \"\"\"\n",
        "    def __init__(self, warmup=0.002, t_total=-1, cycles=1., **kw):\n",
        "        super(WarmupCosineWithHardRestartsSchedule, self).__init__(warmup=warmup, t_total=t_total, cycles=cycles, **kw)\n",
        "        assert(cycles >= 1.)\n",
        "\n",
        "    def get_lr_(self, progress):\n",
        "        if progress < self.warmup:\n",
        "            return progress / self.warmup\n",
        "        else:\n",
        "            progress = (progress - self.warmup) / (1 - self.warmup)     # progress after warmup\n",
        "            ret = 0.5 * (1. + math.cos(math.pi * ((self.cycles * progress) % 1)))\n",
        "            return ret\n",
        "\n",
        "\n",
        "class WarmupCosineWithWarmupRestartsSchedule(WarmupCosineWithHardRestartsSchedule):\n",
        "    \"\"\"\n",
        "    All training progress is divided in `cycles` (default=1.) parts of equal length.\n",
        "    Every part follows a schedule with the first `warmup` fraction of the training steps linearly increasing from 0. to 1.,\n",
        "    followed by a learning rate decreasing from 1. to 0. following a cosine curve.\n",
        "    \"\"\"\n",
        "    def __init__(self, warmup=0.002, t_total=-1, cycles=1., **kw):\n",
        "        assert(warmup * cycles < 1.)\n",
        "        warmup = warmup * cycles if warmup >= 0 else warmup\n",
        "        super(WarmupCosineWithWarmupRestartsSchedule, self).__init__(warmup=warmup, t_total=t_total, cycles=cycles, **kw)\n",
        "\n",
        "    def get_lr_(self, progress):\n",
        "        progress = progress * self.cycles % 1.\n",
        "        if progress < self.warmup:\n",
        "            return progress / self.warmup\n",
        "        else:\n",
        "            progress = (progress - self.warmup) / (1 - self.warmup)     # progress after warmup\n",
        "            ret = 0.5 * (1. + math.cos(math.pi * progress))\n",
        "            return ret\n",
        "\n",
        "\n",
        "class WarmupConstantSchedule(_LRSchedule):\n",
        "    \"\"\"\n",
        "    Linearly increases learning rate from 0 to 1 over `warmup` fraction of training steps.\n",
        "    Keeps learning rate equal to 1. after warmup.\n",
        "    \"\"\"\n",
        "    def get_lr_(self, progress):\n",
        "        if progress < self.warmup:\n",
        "            return progress / self.warmup\n",
        "        return 1.\n",
        "\n",
        "\n",
        "class WarmupLinearSchedule(_LRSchedule):\n",
        "    \"\"\"\n",
        "    Linearly increases learning rate from 0 to 1 over `warmup` fraction of training steps.\n",
        "    Linearly decreases learning rate from 1. to 0. over remaining `1 - warmup` steps.\n",
        "    \"\"\"\n",
        "    warn_t_total = True\n",
        "    def get_lr_(self, progress):\n",
        "        if progress < self.warmup:\n",
        "            return progress / self.warmup\n",
        "        return max((progress - 1.) / (self.warmup - 1.), 0.)\n",
        "\n",
        "\n",
        "SCHEDULES = {\n",
        "    None:       ConstantLR,\n",
        "    \"none\":     ConstantLR,\n",
        "    \"warmup_cosine\": WarmupCosineSchedule,\n",
        "    \"warmup_constant\": WarmupConstantSchedule,\n",
        "    \"warmup_linear\": WarmupLinearSchedule\n",
        "}\n",
        "\n",
        "\n",
        "class BertAdam(Optimizer):\n",
        "    \"\"\"Implements BERT version of Adam algorithm with weight decay fix.\n",
        "    Params:\n",
        "        lr: learning rate\n",
        "        warmup: portion of t_total for the warmup, -1  means no warmup. Default: -1\n",
        "        t_total: total number of training steps for the learning\n",
        "            rate schedule, -1  means constant learning rate of 1. (no warmup regardless of warmup setting). Default: -1\n",
        "        schedule: schedule to use for the warmup (see above).\n",
        "            Can be `'warmup_linear'`, `'warmup_constant'`, `'warmup_cosine'`, `'none'`, `None` or a `_LRSchedule` object (see below).\n",
        "            If `None` or `'none'`, learning rate is always kept constant.\n",
        "            Default : `'warmup_linear'`\n",
        "        b1: Adams b1. Default: 0.9\n",
        "        b2: Adams b2. Default: 0.999\n",
        "        e: Adams epsilon. Default: 1e-6\n",
        "        weight_decay: Weight decay. Default: 0.01\n",
        "        max_grad_norm: Maximum norm for the gradients (-1 means no clipping). Default: 1.0\n",
        "    \"\"\"\n",
        "    def __init__(self, params, lr=required, warmup=-1, t_total=-1, schedule='warmup_linear',\n",
        "                 b1=0.9, b2=0.999, e=1e-6, weight_decay=0.01, max_grad_norm=1.0, **kwargs):\n",
        "        if lr is not required and lr < 0.0:\n",
        "            raise ValueError(\"Invalid learning rate: {} - should be >= 0.0\".format(lr))\n",
        "        if not isinstance(schedule, _LRSchedule) and schedule not in SCHEDULES:\n",
        "            raise ValueError(\"Invalid schedule parameter: {}\".format(schedule))\n",
        "        if not 0.0 <= b1 < 1.0:\n",
        "            raise ValueError(\"Invalid b1 parameter: {} - should be in [0.0, 1.0[\".format(b1))\n",
        "        if not 0.0 <= b2 < 1.0:\n",
        "            raise ValueError(\"Invalid b2 parameter: {} - should be in [0.0, 1.0[\".format(b2))\n",
        "        if not e >= 0.0:\n",
        "            raise ValueError(\"Invalid epsilon value: {} - should be >= 0.0\".format(e))\n",
        "        # initialize schedule object\n",
        "        if not isinstance(schedule, _LRSchedule):\n",
        "            schedule_type = SCHEDULES[schedule]\n",
        "            schedule = schedule_type(warmup=warmup, t_total=t_total)\n",
        "        else:\n",
        "            if warmup != -1 or t_total != -1:\n",
        "                logger.warning(\"warmup and t_total on the optimizer are ineffective when _LRSchedule object is provided as schedule. \"\n",
        "                               \"Please specify custom warmup and t_total in _LRSchedule object.\")\n",
        "        defaults = dict(lr=lr, schedule=schedule,\n",
        "                        b1=b1, b2=b2, e=e, weight_decay=weight_decay,\n",
        "                        max_grad_norm=max_grad_norm)\n",
        "        self.rate = None\n",
        "        super(BertAdam, self).__init__(params, defaults)\n",
        "\n",
        "    def show_lr(self):\n",
        "        return self.rate\n",
        "\n",
        "    def get_lr(self):\n",
        "        lr = []\n",
        "        for group in self.param_groups:\n",
        "            for p in group['params']:\n",
        "                state = self.state[p]\n",
        "                if len(state) == 0:\n",
        "                    return [0]\n",
        "                lr_scheduled = group['lr']\n",
        "                lr_scheduled *= group['schedule'].get_lr(state['step'])\n",
        "                lr.append(lr_scheduled)\n",
        "        return lr\n",
        "\n",
        "    def step(self, closure=None):\n",
        "        \"\"\"Performs a single optimization step.\n",
        "        Arguments:\n",
        "            closure (callable, optional): A closure that reevaluates the model\n",
        "                and returns the loss.\n",
        "        \"\"\"\n",
        "        loss = None\n",
        "        if closure is not None:\n",
        "            loss = closure()\n",
        "\n",
        "        for group in self.param_groups:\n",
        "            for p in group['params']:\n",
        "                if p.grad is None:\n",
        "                    continue\n",
        "                grad = p.grad.data\n",
        "                if grad.is_sparse:\n",
        "                    raise RuntimeError('Adam does not support sparse gradients, please consider SparseAdam instead')\n",
        "\n",
        "                state = self.state[p]\n",
        "\n",
        "                # State initialization\n",
        "                if len(state) == 0:\n",
        "                    state['step'] = 0\n",
        "                    # Exponential moving average of gradient values\n",
        "                    state['next_m'] = torch.zeros_like(p.data)\n",
        "                    # Exponential moving average of squared gradient values\n",
        "                    state['next_v'] = torch.zeros_like(p.data)\n",
        "\n",
        "                next_m, next_v = state['next_m'], state['next_v']\n",
        "                beta1, beta2 = group['b1'], group['b2']\n",
        "\n",
        "                # Add grad clipping\n",
        "                if group['max_grad_norm'] > 0:\n",
        "                    clip_grad_norm_(p, group['max_grad_norm'])\n",
        "\n",
        "                # Decay the first and second moment running average coefficient\n",
        "                # In-place operations to update the averages at the same time\n",
        "                next_m.mul_(beta1).add_(1 - beta1, grad)\n",
        "                next_v.mul_(beta2).addcmul_(1 - beta2, grad, grad)\n",
        "                update = next_m / (next_v.sqrt() + group['e'])\n",
        "\n",
        "                # Just adding the square of the weights to the loss function is *not*\n",
        "                # the correct way of using L2 regularization/weight decay with Adam,\n",
        "                # since that will interact with the m and v parameters in strange ways.\n",
        "                #\n",
        "                # Instead we want to decay the weights in a manner that doesn't interact\n",
        "                # with the m/v parameters. This is equivalent to adding the square\n",
        "                # of the weights to the loss with plain (non-momentum) SGD.\n",
        "                if group['weight_decay'] > 0.0:\n",
        "                    update += group['weight_decay'] * p.data\n",
        "\n",
        "                lr_scheduled = group['lr']\n",
        "                lr_scheduled *= group['schedule'].get_lr(state['step'])\n",
        "\n",
        "                self.rate = lr_scheduled\n",
        "\n",
        "                update_with_lr = lr_scheduled * update\n",
        "                p.data.add_(-update_with_lr)\n",
        "\n",
        "                state['step'] += 1\n",
        "\n",
        "                # step_size = lr_scheduled * math.sqrt(bias_correction2) / bias_correction1\n",
        "                # No bias correction\n",
        "                # bias_correction1 = 1 - beta1 ** state['step']\n",
        "                # bias_correction2 = 1 - beta2 ** state['step']\n",
        "\n",
        "        return loss\n",
        "\n",
        "\n",
        "class Adam(Optimizer):\n",
        "    \"\"\"Implements pytorch version of Adam algorithm with weight decay fix.\n",
        "    Params:\n",
        "        lr: learning rate\n",
        "        warmup: portion of t_total for the warmup, -1  means no warmup. Default: -1\n",
        "        t_total: total number of training steps for the learning\n",
        "            rate schedule, -1  means constant learning rate of 1. (no warmup regardless of warmup setting). Default: -1\n",
        "        schedule: schedule to use for the warmup (see above).\n",
        "            Can be `'warmup_linear'`, `'warmup_constant'`, `'warmup_cosine'`, `'none'`, `None` or a `_LRSchedule` object (see below).\n",
        "            If `None` or `'none'`, learning rate is always kept constant.\n",
        "            Default : `'warmup_linear'`\n",
        "        b1: Adams b1. Default: 0.9\n",
        "        b2: Adams b2. Default: 0.999\n",
        "        e: Adams epsilon. Default: 1e-6\n",
        "        weight_decay: Weight decay. Default: 0.01\n",
        "        max_grad_norm: Maximum norm for the gradients (-1 means no clipping). Default: 1.0\n",
        "    \"\"\"\n",
        "    def __init__(self, params, lr=required, warmup=-1, t_total=-1, schedule='warmup_linear',\n",
        "                 b1=0.9, b2=0.999, e=1e-8, weight_decay=0,  amsgrad=False, max_grad_norm=1.0, **kwargs):\n",
        "        if lr is not required and lr < 0.0:\n",
        "            raise ValueError(\"Invalid learning rate: {} - should be >= 0.0\".format(lr))\n",
        "        if not isinstance(schedule, _LRSchedule) and schedule not in SCHEDULES:\n",
        "            raise ValueError(\"Invalid schedule parameter: {}\".format(schedule))\n",
        "        if not 0.0 <= b1 < 1.0:\n",
        "            raise ValueError(\"Invalid b1 parameter: {} - should be in [0.0, 1.0[\".format(b1))\n",
        "        if not 0.0 <= b2 < 1.0:\n",
        "            raise ValueError(\"Invalid b2 parameter: {} - should be in [0.0, 1.0[\".format(b2))\n",
        "        if not e >= 0.0:\n",
        "            raise ValueError(\"Invalid epsilon value: {} - should be >= 0.0\".format(e))\n",
        "        # initialize schedule object\n",
        "        if not isinstance(schedule, _LRSchedule):\n",
        "            schedule_type = SCHEDULES[schedule]\n",
        "            schedule = schedule_type(warmup=warmup, t_total=t_total)\n",
        "        else:\n",
        "            if warmup != -1 or t_total != -1:\n",
        "                logger.warning(\"warmup and t_total on the optimizer are ineffective when _LRSchedule object is provided as schedule. \"\n",
        "                               \"Please specify custom warmup and t_total in _LRSchedule object.\")\n",
        "        defaults = dict(lr=lr, schedule=schedule,\n",
        "                        b1=b1, b2=b2, e=e, weight_decay=weight_decay,\n",
        "                        amsgrad=amsgrad, max_grad_norm=max_grad_norm)\n",
        "        self.rate = None\n",
        "        super(Adam, self).__init__(params, defaults)\n",
        "\n",
        "    def __setstate__(self, state):\n",
        "        super(Adam, self).__setstate__(state)\n",
        "        for group in self.param_groups:\n",
        "            group.setdefault('amsgrad', False)\n",
        "\n",
        "    def show_lr(self):\n",
        "        return self.rate\n",
        "\n",
        "    def get_lr(self):\n",
        "        lr = []\n",
        "        for group in self.param_groups:\n",
        "            for p in group['params']:\n",
        "                state = self.state[p]\n",
        "                if len(state) == 0:\n",
        "                    return [0]\n",
        "                lr_scheduled = group['lr']\n",
        "                lr_scheduled *= group['schedule'].get_lr(state['step'])\n",
        "                lr.append(lr_scheduled)\n",
        "        return lr\n",
        "\n",
        "    def step(self, closure=None):\n",
        "        \"\"\"Performs a single optimization step.\n",
        "        Arguments:\n",
        "            closure (callable, optional): A closure that reevaluates the model\n",
        "                and returns the loss.\n",
        "        \"\"\"\n",
        "        loss = None\n",
        "        if closure is not None:\n",
        "            loss = closure()\n",
        "\n",
        "        for group in self.param_groups:\n",
        "            for p in group['params']:\n",
        "                if p.grad is None:\n",
        "                    continue\n",
        "                grad = p.grad.data\n",
        "                if grad.is_sparse:\n",
        "                    raise RuntimeError('Adam does not support sparse gradients, please consider SparseAdam instead')\n",
        "                amsgrad = group['amsgrad']\n",
        "\n",
        "                state = self.state[p]\n",
        "\n",
        "                # State initialization\n",
        "                if len(state) == 0:\n",
        "                    state['step'] = 0\n",
        "                    # Exponential moving average of gradient values\n",
        "                    state['exp_avg'] = torch.zeros_like(p.data)\n",
        "                    # Exponential moving average of squared gradient values\n",
        "                    state['exp_avg_sq'] = torch.zeros_like(p.data)\n",
        "                    if amsgrad:\n",
        "                        # Maintains max of all exp. moving avg. of sq. grad. values\n",
        "                        state['max_exp_avg_sq'] = torch.zeros_like(p.data)\n",
        "\n",
        "                exp_avg, exp_avg_sq = state['exp_avg'], state['exp_avg_sq']\n",
        "                if amsgrad:\n",
        "                    max_exp_avg_sq = state['max_exp_avg_sq']\n",
        "                beta1, beta2 = group['b1'], group['b2']\n",
        "\n",
        "                # Add grad clipping\n",
        "                if group['max_grad_norm'] > 0:\n",
        "                    clip_grad_norm_(p, group['max_grad_norm'])\n",
        "\n",
        "                lr_scheduled = group['lr']\n",
        "                lr_scheduled *= group['schedule'].get_lr(state['step'])\n",
        "                self.rate = lr_scheduled\n",
        "\n",
        "                state['step'] += 1\n",
        "\n",
        "                if group['weight_decay'] != 0:\n",
        "                    grad.add_(group['weight_decay'], p.data)\n",
        "\n",
        "                # Decay the first and second moment running average coefficient\n",
        "                exp_avg.mul_(beta1).add_(1 - beta1, grad)\n",
        "                exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\n",
        "                if amsgrad:\n",
        "                    # Maintains the maximum of all 2nd moment running avg. till now\n",
        "                    torch.max(max_exp_avg_sq, exp_avg_sq, out=max_exp_avg_sq)\n",
        "                    # Use the max. for normalizing running avg. of gradient\n",
        "                    denom = max_exp_avg_sq.sqrt().add_(group['e'])\n",
        "                else:\n",
        "                    denom = exp_avg_sq.sqrt().add_(group['e'])\n",
        "\n",
        "                bias_correction1 = 1 - beta1 ** state['step']\n",
        "                bias_correction2 = 1 - beta2 ** state['step']\n",
        "                step_size = lr_scheduled * math.sqrt(bias_correction2) / bias_correction1\n",
        "\n",
        "                p.data.addcdiv_(-step_size, exp_avg, denom)\n",
        "\n",
        "        return loss\n",
        "\n",
        "class Adamax(Optimizer):\n",
        "    \"\"\"Implements Adamax algorithm (a variant of Adam based on infinity norm).\n",
        "    It has been proposed in `Adam: A Method for Stochastic Optimization`__.\n",
        "    Arguments:\n",
        "        params (iterable): iterable of parameters to optimize or dicts defining\n",
        "            parameter groups\n",
        "        lr (float, optional): learning rate (default: 2e-3)\n",
        "        betas (Tuple[float, float], optional): coefficients used for computing\n",
        "            running averages of gradient and its square\n",
        "        eps (float, optional): term added to the denominator to improve\n",
        "            numerical stability (default: 1e-8)\n",
        "        weight_decay (float, optional): weight decay (L2 penalty) (default: 0)\n",
        "    __ https://arxiv.org/abs/1412.6980\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, params, lr=required, warmup=-1, t_total=-1, schedule='warmup_linear',\n",
        "                 b1=0.9, b2=0.999, e=1e-8, weight_decay=0, max_grad_norm=1.0, **kwargs):\n",
        "        if not 0.0 <= lr:\n",
        "            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n",
        "        if not 0.0 <= e:\n",
        "            raise ValueError(\"Invalid epsilon value: {}\".format(e))\n",
        "        if not 0.0 <= b1 < 1.0:\n",
        "            raise ValueError(\"Invalid b1 parameter: {} - should be in [0.0, 1.0[\".format(b1))\n",
        "        if not 0.0 <= b2 < 1.0:\n",
        "            raise ValueError(\"Invalid b2 parameter: {} - should be in [0.0, 1.0[\".format(b2))\n",
        "        if not 0.0 <= weight_decay:\n",
        "            raise ValueError(\"Invalid weight_decay value: {}\".format(weight_decay))\n",
        "\n",
        "        if not isinstance(schedule, _LRSchedule):\n",
        "            schedule_type = SCHEDULES[schedule]            \n",
        "            schedule = schedule_type(warmup=warmup, t_total=t_total)\n",
        "        else:\n",
        "            if warmup != -1 or t_total != -1:\n",
        "                logger.warning(\"warmup and t_total on the optimizer are ineffective when _LRSchedule object is provided as schedule. \"\n",
        "                               \"Please specify custom warmup and t_total in _LRSchedule object.\")\n",
        "\n",
        "        defaults = dict(lr=lr, schedule=schedule,\n",
        "                        b1=b1, b2=b2, e=e, weight_decay=weight_decay, max_grad_norm=max_grad_norm)\n",
        "        self.rate = None\n",
        "        super(Adamax, self).__init__(params, defaults)\n",
        "\n",
        "    def show_lr(self):\n",
        "        return self.rate\n",
        "\n",
        "    def get_lr(self):\n",
        "        lr = []\n",
        "        for group in self.param_groups:\n",
        "            for p in group['params']:\n",
        "                state = self.state[p]\n",
        "                if len(state) == 0:\n",
        "                    return [0]\n",
        "                lr_scheduled = group['lr']\n",
        "                lr_scheduled *= group['schedule'].get_lr(state['step'])\n",
        "                lr.append(lr_scheduled)\n",
        "        return lr\n",
        "\n",
        "    def step(self, closure=None):\n",
        "        \"\"\"Performs a single optimization step.\n",
        "        Arguments:\n",
        "            closure (callable, optional): A closure that reevaluates the model\n",
        "                and returns the loss.\n",
        "        \"\"\"\n",
        "        loss = None\n",
        "        if closure is not None:\n",
        "            loss = closure()\n",
        "\n",
        "        for group in self.param_groups:\n",
        "            for p in group['params']:\n",
        "                if p.grad is None:\n",
        "                    continue\n",
        "                grad = p.grad.data\n",
        "                if grad.is_sparse:\n",
        "                    raise RuntimeError('Adamax does not support sparse gradients')\n",
        "                state = self.state[p]\n",
        "\n",
        "                # State initialization\n",
        "                if len(state) == 0:\n",
        "                    state['step'] = 0\n",
        "                    state['exp_avg'] = torch.zeros_like(p.data)\n",
        "                    state['exp_inf'] = torch.zeros_like(p.data)\n",
        "\n",
        "                exp_avg, exp_inf = state['exp_avg'], state['exp_inf']\n",
        "                beta1, beta2 = group['b1'], group['b2']\n",
        "                eps = group['e']\n",
        "\n",
        "                # Add grad clipping\n",
        "                if group['max_grad_norm'] > 0:\n",
        "                    clip_grad_norm_(p, group['max_grad_norm'])\n",
        "\n",
        "                lr_scheduled = group['lr']\n",
        "                lr_scheduled *= group['schedule'].get_lr(state['step'])\n",
        "\n",
        "                self.rate = lr_scheduled\n",
        "                state['step'] += 1\n",
        "\n",
        "                if group['weight_decay'] != 0:\n",
        "                    grad = grad.add(group['weight_decay'], p.data)\n",
        "\n",
        "                # Update biased first moment estimate.\n",
        "                exp_avg.mul_(beta1).add_(1 - beta1, grad)\n",
        "                # Update the exponentially weighted infinity norm.\n",
        "                norm_buf = torch.cat([\n",
        "                    exp_inf.mul_(beta2).unsqueeze(0),\n",
        "                    grad.abs().add_(eps).unsqueeze_(0)\n",
        "                ], 0)\n",
        "                torch.max(norm_buf, 0, keepdim=False, out=(exp_inf, exp_inf.new().long()))\n",
        "\n",
        "                bias_correction = 1 - beta1 ** state['step']\n",
        "                clr = lr_scheduled / bias_correction\n",
        "\n",
        "                p.data.addcdiv_(-clr, exp_avg, exp_inf)\n",
        "\n",
        "        return loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "14Qm-iHYJGM-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "tokenizer = T5Tokenizer.from_pretrained('t5-small')\n",
        "model = T5ForConditionalGeneration.from_pretrained('t5-small')\n",
        "model = model.to(device)\n",
        "# optimizer should be defined outside training loop\n",
        "# optimizer = optim.SGD(model.parameters(), lr=0.01)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VScZuyPaJGNF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "num_epoch = 1\n",
        "no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n",
        "optimizer_grouped_parameters = []\n",
        "lr=2e-5\n",
        "for key, value in dict(model.named_parameters()).items():\n",
        "    if value.requires_grad:\n",
        "        if any(nd in key for nd in no_decay):\n",
        "            optimizer_grouped_parameters += [\n",
        "                {\"params\": [value], \"lr\": lr, \"weight_decay\": 0.01}\n",
        "            ]\n",
        "        if not any(nd in key for nd in no_decay):\n",
        "            optimizer_grouped_parameters += [\n",
        "                {\"params\": [value], \"lr\": lr, \"weight_decay\": 0.0}\n",
        "            ]\n",
        "                \n",
        "optimizer = BertAdam(\n",
        "            optimizer_grouped_parameters,\n",
        "            lr=lr,\n",
        "            warmup=0.1,\n",
        "            t_total=100,\n",
        "#             t_total=num_epoch * len(training_generator),\n",
        "            schedule='warmup_constant',\n",
        "        )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "6zQaAWNlcBK-",
        "outputId": "873701c6-0912-4986-823b-f6940213ee65",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 903,
          "referenced_widgets": [
            "c06fa29ce6f341338b8d59ec1436a874",
            "aad42e1fd8d14b80960e754091958526",
            "433508b5bbc54ddd95c33ad17351877f",
            "c784e079f0d24752b5faa3dabe8d9863",
            "7e023fef2d9047d58e8a83d4f549949d",
            "e59b2e875a7640c2a250b2877bdbcb5e",
            "77244fe9ab54444b86812fec92ea5890",
            "426faa8edb8246429a4f93bd89e44df0"
          ]
        }
      },
      "source": [
        "\n",
        "loss_train = []\n",
        "loss_valid = []\n",
        "\"\"\"validloss = 0\n",
        "for local_batch, local_labels in tqdm(validation_generator):\n",
        "    input_ids = tokenizer.batch_encode_plus(local_batch, return_tensors=\"pt\",pad_to_max_length=True)\n",
        "    label = tokenizer.batch_encode_plus(local_labels, return_tensors=\"pt\",pad_to_max_length=True)\n",
        "    model.eval()\n",
        "    outputs = model(input_ids=(input_ids['input_ids']).to(device), lm_labels=(label['input_ids']).to(device))\n",
        "    loss, prediction_scores = outputs[:2]\n",
        "    validloss += loss\n",
        "loss_valid.append(validloss / len(validation_generator))\n",
        "\"\"\"\n",
        "for epoch in range(num_epoch):\n",
        "    trainloss=0\n",
        "    validloss=0\n",
        "    # Training\n",
        "    for local_batch, local_labels in tqdm(training_generator): \n",
        "        model.zero_grad()\n",
        "        \"\"\"Forward Function Implementation\"\"\"\n",
        "        # Also, put the encode function in dataset. Return encoded matrix values from dataset. Here, only take it to gpu.\n",
        "        input_ids = tokenizer.batch_encode_plus(local_batch, return_tensors=\"pt\",pad_to_max_length=True)\n",
        "        label = tokenizer.batch_encode_plus(local_labels, return_tensors=\"pt\",pad_to_max_length=True)\n",
        "        outputs = model(input_ids=(input_ids['input_ids']).to(device), lm_labels=(label['input_ids']).to(device),attention_mask=(input_ids['attention_mask']).to(device))\n",
        "        loss, prediction_scores = outputs[:2]\n",
        "        trainloss=loss\n",
        "        \"\"\"Forward Function Ends here\"\"\"\n",
        "        loss_train.append(trainloss)\n",
        "        \"\"\"Loss and optimizer\"\"\"\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    \n",
        "\n",
        "\n",
        "    \"\"\"Validation\"\"\"\n",
        "    with torch.set_grad_enabled(False):\n",
        "        for local_batch, local_labels in tqdm(validation_generator):\n",
        "          input_ids = tokenizer.batch_encode_plus(local_batch, return_tensors=\"pt\",pad_to_max_length=True)\n",
        "          label = tokenizer.batch_encode_plus(local_labels, return_tensors=\"pt\",pad_to_max_length=True)\n",
        "          model.eval()\n",
        "          outputs = model(input_ids=(input_ids['input_ids']).to(device), lm_labels=(label['input_ids']).to(device))\n",
        "          loss, prediction_scores = outputs[:2]\n",
        "          validloss += loss\n",
        "    loss_valid.append(validloss / len(validation_generator))\n",
        "    print(\"\\nEpoch \", epoch, \" completed!, Training LOSS is: \", trainloss, \" Validation loss is: \", validloss/4)\n",
        "#plt.plot(loss_train)\n",
        "#plt.plot(loss_valid)\n",
        "    #print(loss, \"\\n\\n\")\n",
        "model_name = \"/content/drive/My Drive/Transformer/models/model.pth\"\n",
        "\n",
        "torch.save(model.state_dict(), model_name)\n"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c06fa29ce6f341338b8d59ec1436a874",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=0, max=89163), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-24-24b9f22e0eed>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0mloss_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0;34m\"\"\"Loss and optimizer\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    196\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m         \"\"\"\n\u001b[0;32m--> 198\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     98\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     99\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 360.00 MiB (GPU 0; 11.17 GiB total capacity; 9.99 GiB already allocated; 184.81 MiB free; 10.63 GiB reserved in total by PyTorch) (malloc at /pytorch/c10/cuda/CUDACachingAllocator.cpp:289)\nframe #0: c10::Error::Error(c10::SourceLocation, std::string const&) + 0x46 (0x7feb4b05a536 in /usr/local/lib/python3.6/dist-packages/torch/lib/libc10.so)\nframe #1: <unknown function> + 0x1cf1e (0x7feb4b2a3f1e in /usr/local/lib/python3.6/dist-packages/torch/lib/libc10_cuda.so)\nframe #2: <unknown function> + 0x1df9e (0x7feb4b2a4f9e in /usr/local/lib/python3.6/dist-packages/torch/lib/libc10_cuda.so)\nframe #3: at::native::empty_cuda(c10::ArrayRef<long>, c10::TensorOptions const&, c10::optional<c10::MemoryFormat>) + 0x135 (0x7feb4de50fd5 in /usr/local/lib/python3.6/dist-packages/torch/lib/libtorch_cuda.so)\nframe #4: <unknown function> + 0xf9310b (0x7feb4c44910b in /usr/local/lib/python3.6/dist-packages/torch/lib/libtorch_cuda.so)\nframe #5: <unknown function> + 0xfdc9f7 (0x7feb4c4929f7 in /usr/local/lib/python3.6/dist-packages/torch/lib/libtorch_cuda.so)\nframe #6: <unknown function> + 0x1075389 (0x7feb83ddd389 in /usr/local/lib/python3.6/dist-packages/torch/lib/libtorch_cpu.so)\nframe #7: <unknown function> + 0x10756c7 (0x7feb83ddd6c7 in /usr/local/lib/python3.6/dist-packages/torch/lib/libtorch_cpu.so)\nframe #8: <unknown function> + 0xe2165e (0x7feb83b8965e in /usr/local/lib/python3.6/dist-packages/torch/lib/libtorch_cpu.so)\nframe #9: at::native::empty_like(at::Tensor const&, c10::TensorOptions const&, c10::optional<c10::MemoryFormat>) + 0x9e0 (0x7feb83b8ff50 in /usr/local/lib/python3.6/dist-packages/torch/lib/libtorch_cpu.so)\nframe #10: <unknown function> + 0x1134321 (0x7feb83e9c321 in /usr/local/lib/python3.6/dist-packages/torch/lib/libtorch_cpu.so)\nframe #11: <unknown function> + 0x1187623 (0x7feb83eef623 in /usr/local/lib/python3.6/dist-packages/torch/lib/libtorch_cpu.so)\nframe #12: <unknown function> + 0x28c7d12 (0x7feb4dd7dd12 in /usr/local/lib/python3.6/dist-packages/torch/lib/libtorch_cuda.so)\nframe #13: at::Tensor at::native::(anonymous namespace)::host_softmax_backward<at::native::(anonymous namespace)::SoftMaxBackwardEpilogue, false>(at::Tensor const&, at::Tensor const&, long, bool) + 0xb9 (0x7feb4dd94529 in /usr/local/lib/python3.6/dist-packages/torch/lib/libtorch_cuda.so)\nframe #14: at::native::softmax_backward_cuda(at::Tensor const&, at::Tensor const&, long, at::Tensor const&) + 0x19c (0x7feb4dd7e7dc in /usr/local/lib/python3.6/dist-packages/torch/lib/libtorch_cuda.so)\nframe #15: <unknown function> + 0xfa4940 (0x7feb4c45a940 in /usr/local/lib/python3.6/dist-packages/torch/lib/libtorch_cuda.so)\nframe #16: <unknown function> + 0x10c5ad6 (0x7feb83e2dad6 in /usr/local/lib/python3.6/dist-packages/torch/lib/libtorch_cpu.so)\nframe #17: <unknown function> + 0x2b4dd6c (0x7feb858b5d6c in /usr/local/lib/python3.6/dist-packages/torch/lib/libtorch_cpu.so)\nframe #18: <unknown function> + 0x10c5ad6 (0x7feb83e2dad6 in /usr/local/lib/python3.6/dist-packages/torch/lib/libtorch_cpu.so)\nframe #19: torch::autograd::generated::SoftmaxBackward::apply(std::vector<at::Tensor, std::allocator<at::Tensor> >&&) + 0x1c9 (0x7feb8560db79 in /usr/local/lib/python3.6/dist-packages/torch/lib/libtorch_cpu.so)\nframe #20: <unknown function> + 0x2d89c05 (0x7feb85af1c05 in /usr/local/lib/python3.6/dist-packages/torch/lib/libtorch_cpu.so)\nframe #21: torch::autograd::Engine::evaluate_function(std::shared_ptr<torch::autograd::GraphTask>&, torch::autograd::Node*, torch::autograd::InputBuffer&) + 0x16f3 (0x7feb85aeef03 in /usr/local/lib/python3.6/dist-packages/torch/lib/libtorch_cpu.so)\nframe #22: torch::autograd::Engine::thread_main(std::shared_ptr<torch::autograd::GraphTask> const&, bool) + 0x3d2 (0x7feb85aefce2 in /usr/local/lib/python3.6/dist-packages/torch/lib/libtorch_cpu.so)\nframe #23: torch::autograd::Engine::thread_init(int) + 0x39 (0x7feb85ae8359 in /usr/local/lib/python3.6/dist-packages/torch/lib/libtorch_cpu.so)\nframe #24: torch::autograd::python::PythonEngine::thread_init(int) + 0x38 (0x7feb92227378 in /usr/local/lib/python3.6/dist-packages/torch/lib/libtorch_python.so)\nframe #25: <unknown function> + 0xbd6df (0x7febbaa026df in /usr/lib/x86_64-linux-gnu/libstdc++.so.6)\nframe #26: <unknown function> + 0x76db (0x7febbbae46db in /lib/x86_64-linux-gnu/libpthread.so.0)\nframe #27: clone + 0x3f (0x7febbbe1d88f in /lib/x86_64-linux-gnu/libc.so.6)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5pxbcJIEhUcL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.plot(loss_train)\n",
        "plt.show()\n",
        "plt.plot(loss_valid)\n",
        "states = {\n",
        "            'epoch': epoch + 1,\n",
        "            'state_dict': model.state_dict(),\n",
        "            'optimizer': optimizer.state_dict(),\n",
        "        }\n",
        "torch.save(states, save_file_path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "VMdN-HAVDuGm",
        "colab": {}
      },
      "source": [
        " model.eval()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "pVOZGflPBMIk",
        "colab": {}
      },
      "source": [
        "\"\"\"temp_index=50 \n",
        "ip=tokenizer.encode(X[temp_index], return_tensors=\"pt\")\n",
        "op=model.generate(input_ids=ip.to(device))\n",
        "print(tokenizer.decode(op[0]))\n",
        "print(X[temp_index])\n",
        "print(Y[temp_index])\"\"\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "oXLsw4qsNmPq",
        "colab": {}
      },
      "source": [
        "\"\"\"from datetime import datetime as dt\n",
        "runtime =  dt.now()\n",
        "model_name = \"/content/drive/My Drive/Transformer/models/\" + runtime.strftime(\"%d:%m:%Y:%H:%M\") + \".pth\"\n",
        "\n",
        "torch.save(model.state_dict(), model_name)\"\"\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mfdqyNDBIKxE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}